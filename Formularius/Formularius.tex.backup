\documentclass[10pt]{article}
\usepackage[letterpaper,textheight=60em]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{array}
\usepackage[textwidth=8em,textsize=small]{todonotes}
\usepackage{amsmath}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}
\usepackage{lineno}
\makeatletter
% Make a copy of macros responsible for entering display math mode
\let\start@align@nopar\start@align
\let\start@gather@nopar\start@gather
\let\start@multline@nopar\start@multline
% Add the "empty line" command to the macros
\long\def\start@align{\par\start@align@nopar}
\long\def\start@gather{\par\start@gather@nopar}
\long\def\start@multline{\par\start@multline@nopar}
\makeatother

%% Added by the authors
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsfonts, amssymb, dsfont}
\usepackage{enumerate}
\usepackage{tikz}
\usetikzlibrary{calc,shapes,backgrounds,arrows,automata,shadows,positioning}
\usepackage[ruled]{algorithm2e}

\usepackage{float}
\usepackage{multirow}
\usepackage{url}
\usepackage{array}
\usepackage{etoolbox}

%% somes macros
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\1}{\mathds{1}}
\newcommand{\MA}{Y}
\newcommand{\MAO}{\MA^\text{\rm o}}
\newcommand{\MAM}{\MA^\text{\rm m}}
\newcommand{\bMAM}{\mathbf{\MA}^\text{\rm m}}
\newcommand{\bMAO}{\mathbf{\MA}^\text{\rm o}}
\newcommand{\card}[1]{\text{\rm card}\left(#1\right)}

%% sets in SBM
\newcommand{\block}{\mathcal{Q}}

\newcommand{\dyad}{\mathcal{D}}
\newcommand{\dyadO}{\dyad^\text{\rm o}}
\newcommand{\dyadM}{\dyad^\text{\rm m}}

\newcommand{\node}{\mathcal{N}}
\newcommand{\nodeO}{\node^\text{\rm o}}
\newcommand{\nodeM}{\node^\text{\rm m}}

\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Rbb}{\mathbb{R}}

\newcommand{\argmax}{\mathop{\mathrm{arg\ max}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\ min}}}


%macros
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bY}{\mathbf{Y}}

\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{property}{Property}
\newtheorem*{properties*}{Properties}
\newtheorem*{lemma*}{Lemma}



%opening
\title{Formularius}
\author{}
\date{}

\begin{document}
\maketitle
\tableofcontents

\section{Binary SBM}
\subsection{Undirected/\textcolor{red}{Directed}}

\subsubsection{MAR}
\textcolor{red}{In the directed case, only $\dyadO$ is changing in the following (i.e don't forget that the matrix is not symmetric: $\MA_{ij}$
and $\MA_{ji}$ are two different random variables).}
\begin{proposition}\label{prop:complete_loglik_sbm}      The      complete
  log-likelihood restricted to the observed variables is
  \begin{equation}
    \nonumber
    \log  p_{\theta}(\MAO,Z)=  \sum_{(i,j)\in  \dyadO}  \sum_{(q,\ell)
      \in\block^2}   Z_{iq}Z_{j\ell}\log   b(\MA_{ij},\pi_{q\ell})   +
    \sum_{i\in \nodeO}\sum_{q\in\block} Z_{iq}\log (\alpha_{q}),
    % \label{loglik_obs_mar}
  \end{equation}
  with $b(x,\pi)=\pi^{x}(1-\pi)^{1-x}$ the Bernoulli probability density function.
\end{proposition}

\begin{proposition} 
The variationnal approximation is
\begin{equation*}
  \label{eq:J_MAR}
  J_{\tau,\theta}(\MAO) =  \sum_{(i,j)\in  \dyadO}  \sum_{(q,\ell)\in\block^2}  \tau_{iq}\tau
  _{j\ell}\log       b(\MA_{ij},\pi_{q\ell})      +       \sum_{i\in\nodeO}\sum_{q\in\block} \tau _{iq}\log (\alpha_{q}/\tau_{iq}).
\end{equation*}
\end{proposition}

\begin{proposition} Consider  the lower  bound $J_{\tau,\theta}(\MAO)$.
  \begin{enumerate}
  \item   The  parameters $\theta=(\alpha,  \pi)$ maximizing  $J_\theta(\MAO)$
    when $\tau$ is held fixed are
    \begin{equation}\nonumber
      \hat{\alpha}_q=\frac{\sum_{i\in \nodeO}\hat{\tau}_{iq}}{\card{\nodeO}}, \qquad
      \hat{\pi}_{q\ell}=\frac{\sum_{(i,j)\in
          \dyadO}\hat{\tau}_{iq}\hat{\tau}_{j\ell}\MA_{ij}}{\sum_{(i,j)\in
          \dyadO
        }\hat{\tau}_{iq}\hat{\tau}_{j\ell}}.
    \end{equation}
  \item   The  variational parameters  $\tau$  maximizing $J_\tau(\MAO)$  when
    $\theta$ is held fixed are obtained thanks to the following fixed point relation:
    \begin{equation}\nonumber
      \hat{\tau}_{iq}\propto \alpha_{q} \left( \prod_{(i,j)\in\dyadO}
        \prod_{\ell\in\block} b(\MA_{ij}; \pi_{q\ell})^{\hat{\tau}_{j\ell}}\right).
    \end{equation}
        \textcolor{red}{    \begin{equation}\nonumber
      \hat{\tau}_{iq}\propto \alpha_{q} \left( \prod_{(i,j)\in\dyadO}
        \prod_{\ell\in\block} (b(\MA_{ij}; \pi_{q\ell})b(\MA_{ji}; \pi_{\ell q}))^{\hat{\tau}_{j\ell}}\right).
    \end{equation}}
  \end{enumerate}
\end{proposition}

\begin{proposition}
\label{prop:ICL_MAR}
  For      an      SBM      with     $Q$      blocks      and      for
  $\hat{\theta}=\argmax \log p_{\theta}(\MAO,Z)$, the ICL criterion is
  given by
  \begin{equation}\nonumber
    \mathrm{ICL}(Q)   =    -2   \mathbb{E}_{\tilde   p_\tau}\left[\log
      p_{\hat\theta}(\MAO,  Z   ;  Q)\right]   +  \frac{Q(Q+1)}{2}\log
    \card{\dyadO} + (Q-1)\log \card{\nodeO}. 
  \end{equation}
  \textcolor{red}{  \begin{equation}\nonumber
    \mathrm{ICL}(Q)   =    -2   \mathbb{E}_{\tilde   p_\tau}\left[\log
      p_{\hat\theta}(\MAO,  Z   ;  Q)\right]   +  Q^2\log
    \card{\dyadO} + (Q-1)\log \card{\nodeO}. 
  \end{equation}}
\end{proposition}

\paragraph{Likelihood of Random-Pair sampling}
\begin{equation*}
p_{\rho}(R)=\left( \prod_{\MAO}\rho \right) \left( \prod_{\MAM}(1-\rho) \right)
\end{equation*}

\subsubsection{NMAR}
\textcolor{red}{Notice that $\dyadO$ and $\dyadM$ are changing in the following (i.e don't forget that the matrix is not symmetric: $\MA_{ij}$
and $\MA_{ji}$ are two different random variables).}
\begin{proposition}      The      complete
  log-likelihood is 
\begin{equation*}
\log  p_{\theta, \psi}(\MAO,R,\MAM,Z)  = \log  p_\psi(R|\MAO,\MAM,Z) +
\log p_\theta(\MAO,\MAM,Z),
 \label{eq:complete_loglik_nmar}
\end{equation*}
where
  \begin{equation}
    \nonumber
    \log  p_{\theta}(\MA,Z)=  \sum_{1 \leq i < j \leq n}  \sum_{(q,\ell)
      \in\block^2}   Z_{iq}Z_{j\ell}\log   b(\MA_{ij},\pi_{q\ell})   +
    \sum_{i\in \{ 1,...n \}}\sum_{q\in\block} Z_{iq}\log (\alpha_{q}),
  \end{equation}
  \textcolor{red}{\begin{equation}
    \nonumber
    \log  p_{\theta}(\MA,Z)=  \sum_{1 \leq i \neq j \leq n}  \sum_{(q,\ell)
      \in\block^2}   Z_{iq}Z_{j\ell}\log   b(\MA_{ij},\pi_{q\ell})   +
    \sum_{i\in \{ 1,...n \}}\sum_{q\in\block} Z_{iq}\log (\alpha_{q}),
  \end{equation}
}
  with $b(x,\pi)=\pi^{x}(1-\pi)^{1-x}$ the Bernoulli probability density function.
\end{proposition}

\begin{proposition} 
The variationnal approximation is
\begin{eqnarray*}
  J_{\tau,\nu,\theta,\psi}(\MAO,R) 
  &=& \Ebb_{\tilde       p_{\tau,\nu}}       \left[\log
      p_{\theta,\psi}(\MAO,R,\MAM,Z)\right]
    -\Ebb_{\tilde p_{\tau,\nu}}\left[ \log \tilde p_{\tau,\nu}(Z,\MAM)\right] \hspace{3cm} (1) \\
  &=& \Ebb_{\tilde p_{\tau,\nu}} \left[\log p_\psi(R | \MAO,\MAM,Z)\right] \\  
  && + \sum_{(i,j)\in\dyadO}\sum_{(q,\ell)\in\block^2}\tau_{iq}\tau_{j\ell}\log
  b(\MA_{ij},\pi_{q\ell})
  +\sum_{(i,j)\in\dyadM}\sum_{(q,\ell)\in\block^2}\tau_{iq}\tau_{j\ell}\log
  b(\nu_{ij},\pi_{q\ell}) \\
  && +  \sum_{i\in\node}\sum_{q\in\block}\tau_{iq} \log (\alpha_{q}/\tau_{iq})
  -\sum_{(i,j)\in \dyadM} \nu_{ij}\log (\nu_{ij}) + (1-\nu_{ij}) \log (1-\nu_{ij}).
  \label{eq:var:approx:nmar}
\end{eqnarray*}
\end{proposition}

\begin{proposition}\label{prop:nmar_common}
  Consider   the  lower   bound  $J_{\tau, \nu, \theta, \psi}(\MAO,R)$   given  by
  \eqref{eq:var:approx:nmar}.
  \begin{enumerate}
  \item The parameters $\theta=(\alpha, \pi)$ maximizing 
    \eqref{eq:var:approx:nmar} when all other parameters are held fixed are
    \begin{equation}\nonumber
      \hat{\alpha}_q=\frac{1}{n}\sum_{i\in\node} \hat{\tau}_{iq}, \qquad
      \hat{\pi}_{q\ell}=\frac{\sum_{(i,j)\in
          \dyadO}\hat{\tau}_{iq}\hat{\tau}_{j\ell}\MA_{ij}           +
        \sum_{(i,j)\in
          \dyadM}\hat{\tau}_{iq}\hat{\tau}_{j\ell}\hat{\nu}_{ij}}{\sum_{(i,j)\in\dyad}\hat{\tau}_{iq}\hat{\tau}_{j\ell}}.      
    \end{equation}
  \item  The optimal  $\tau$  in  \eqref{eq:var:approx:nmar} when  all
    other parameters are held fixed verifies
    \begin{equation}\nonumber
      \hat{\tau}_{iq}\propto \lambda_{iq} \alpha_{q} \left( \prod_{(i,j)\in \dyadO}
        \prod_{\ell\in\block}                                       b(\MA_{ij};
        \pi_{q\ell})^{\hat{\tau}_{j\ell}}\right)
      \left( \prod_{(i,j)\in \dyadM}
        \prod_{\ell\in\block} b(\nu_{ij}; \pi_{q\ell})^{\hat{\tau}_{j\ell}}\right).
    \end{equation}
        \textcolor{red}{    \begin{equation} \nonumber
      \hat{\tau}_{iq}\propto \lambda_{iq} \alpha_{q} \left( \prod_{(i,j)\in \dyadO}
        \prod_{\ell\in\block} (b(\MA_{ij}; \pi_{q\ell})b(\MA_{ji}; \pi_{\ell q}))^{\hat{\tau}_{j\ell}}\right)
      \left( \prod_{(i,j)\in \dyadM}
        \prod_{\ell\in\block} (b(\nu_{ij}; \pi_{q\ell})b(\nu_{ji}; \pi_{\ell q}))^{\hat{\tau}_{j\ell}}\right).
    \end{equation}}
    with $\lambda_{iq}$ a simple constant depending on the sampling design.
  \end{enumerate}
\end{proposition}

\paragraph*{\bf     Double-standard     sampling.}     Recall     that
$S^\text{\rm o}  = \sum_{(i,j)\in \dyadO} Y_{ij},  \ \bar{S}^\text{\rm
  o}        =        \sum_{(i,j)\in        \dyadO}        (1-Y_{ij})$,
and                                                             denote
$S^\text{\rm m} = \sum_{(i,j)\in \dyadM} \nu_{ij}, \ \bar{S}^\text{\rm
  m}        =       \sum_{(i,j)\in        \dyadM}       (1-\nu_{ij})$.
Regarding  the   likelihood  of   the  double
standard sampling, we have
\begin{equation}\nonumber
  \mathbb{E}_{\tilde p} \log p_{\psi}(R|\MA) =
  S^\text{\rm o} \log \rho_1 + \bar{S}^\text{\rm o}
  \log \rho_0 + S^\text{\rm m}  \log (1-\rho_1) + \bar{S}^\text{\rm m}
  \log (1-\rho_0).
\end{equation}
Based on this expression, we easily derive the following proposition: 
\begin{proposition}\label{prop:nmar_doublestand}
  Consider     the     maximization     of     the     lower     bound
  \eqref{eq:var:approx:nmar} in the double standard sampling.
  \begin{enumerate}
  \item   The   parameters   $\psi   =   (\rho_0,\rho_1)$   maximizing
    \eqref{eq:var:approx:nmar}  when  all  other parameters  are  held
    fixed are
    \begin{equation}
      \label{eq:rho_01}
      \hat{\rho}_0  = \frac{\bar{S}^\text{\rm  o}}{\bar{S}^\text{\rm o}  +
        \bar{S}^\text{\rm m}}, \qquad
      \hat{\rho}_1 = \frac{S^\text{\rm o}}{S^\text{\rm o} + S^\text{\rm m}}.
    \end{equation}
    
  \item The optimal $\nu$ in \eqref{eq:var:approx:nmar} when all
    other parameters are held fixed are
    \begin{equation}\nonumber
      \hat{\nu}_{ij} = \mathrm{logistic} \left( \log \left( \frac{1-\rho_1}{1-\rho_0} \right) + \sum_{(q,\ell)\in\block^2} \tau_{iq}\tau_{j\ell} \log\left(\frac{\pi_{q\ell}}{1-\pi_{q\ell}}\right) \right).
    \end{equation}
  \end{enumerate}
  Moreover, $\lambda_{iq}  = 1 \ \forall  (i,q) \in \node\times\block$
  for optimization of $\tau$ in Proposition~\ref{prop:nmar_common}.2).
\end{proposition}

\paragraph*{\bf Class sampling.}  According to the
likelihood of the  class sampling, we derive
the  following expression  of the  conditional expectation  under the
variational approximation:
\begin{equation}\nonumber
  \mathbb{E}_{\tilde{p}} \log p_\psi(R|\MA)  = \sum_{i\in \nodeO}
  \sum_{q\in\block} \tau_{iq} \log(\rho_q) + 
  \sum_{i\in \nodeM}\sum_{q\in\block} \tau_{iq}\log(1-\rho_q),
\end{equation}
from which we derive the  maximization of the remaining parameters for
class sampling.

\begin{proposition}\label{prop:nmar_class}
  Consider     the     maximization     of     the     lower     bound
  \eqref{eq:var:approx:nmar} in the class sampling.
  \begin{enumerate}
  \item  The  parameters  $\psi  =  (\rho_1,\dots,\rho_Q)$  maximizing
    \eqref{eq:var:approx:nmar}  when  all  other parameters  are  held
    fixed are
    \begin{equation}
      \label{eq:rho_q}
      \hat{\rho}_q    =   \frac{\sum_{i\in\nodeO}\tau_{iq}}{\sum_{i\in\node}\tau_{iq}}.
    \end{equation}
  \item The optimal $\nu$ in \eqref{eq:var:approx:nmar} when all
    other parameters are held fixed verify
    \begin{equation}\nonumber
      \hat{\nu}_{ij} = \mathrm{logistic} \left( \sum_{(q,\ell)\in\block^2} \tau_{iq}\tau_{j\ell} \log\left(\frac{\pi_{q\ell}}{1-\pi_{q\ell}}\right) \right).
    \end{equation}
    
  \end{enumerate}
  Moreover   $\lambda_{iq}  =   \rho_q^{\1_{\{i\in\nodeO\}}}(1-\rho_q)^{^{\1_{\{i\in\nodeM\}}}}$  for
  optimization of $\tau$ in Proposition~\ref{prop:nmar_common}.2).
\end{proposition}

\paragraph*{\bf Star degree sampling.}  From
Expression of the  likelihood for star degree
sampling, one has
\begin{equation}\nonumber
  \mathbb{E}_{\tilde{p}} \log p_{\psi}(R|\MA)
  = -\sum_{i\in\nodeM} \left(a+b\tilde{D}_i \right)  + \sum_{i\in\node}  \Ebb_{\tilde{p}}\left[ -\log(1+e^{-(a+bD_i)}) \right],
\end{equation}
where
$\tilde{D}_i  =  \mathbb{E}_{\tilde{p}}\left[D_i   \right]  =  \sum_{i
  \in\nodeM}   \nu_{ij}  +   \sum_{i  \in\nodeO}   \MA_{ij}$  is   the
approximation of the degrees.

Because $\mathbb{E}_{\tilde{p}}\left[  -\log(1+e^{-(a+bD_i)}) \right]$
has  no   explicit  form,  we   rely  on  an   additional  variational
approximation.   The principle  is as  follows: since
$g(x) =  -\log(1+e^{-x})$ is  a convex function,  we have  from Taylor
expansion
\begin{equation}\nonumber
g(x) \geq g(\zeta) + \frac{x-\zeta}{2} + h(\zeta)(x^2
- \zeta^2), \ \forall (x,\zeta) \in \mathbb{R}\times\mathbb{R}^{+}, 
\end{equation}
where
$h(x) =  \frac{-1}{2\zeta}\left[ \text{logistic}(\zeta)  - \frac{1}{2}
\right]$ .  This leads to a lower bound of the initial lower bound:
\begin{equation}
  \label{eq:lower_bound_degree}
  \log p_{\theta,\psi}(\MAO,R)  \geq
  J_{\tau,\nu,\theta,\psi}(\MAO,R) \geq
  J_{\tau,\nu,\zeta,\theta,\psi}(\MAO,R),
\end{equation}
with $\zeta = \left(  \zeta_i, i\in\node\right)$ such that $\zeta_i>0$
is an  additional set  of variational  parameters used  to approximate
$-\log(1+e^{-x})$.

\begin{proposition}\label{prop:nmar_degree}
  Consider     the     maximization     of     the     lower     bound
  \eqref{eq:lower_bound_degree} in the star degree sampling. 
  Let us denote $\hat{D}_i =\Ebb_{\tilde{p}}\left[ {D_i}^2 \right]$ and $\tilde{D}_{k}^{-\ell} = \tilde{D}_k - \nu_{k\ell}$.
  \begin{enumerate}
  \item The parameters $\psi = (a,b)$ maximizing $J_{\tau,\nu,\zeta,\theta,\psi}(\MAO,R)$  when  all  other parameters  are  held
    fixed are
    \begin{align}\nonumber
      \hat{b} &= \frac{  2\left(\frac{n}{2}-\card{\nodeM} \right)\sum_{i=1}^n (h(\zeta_i) \tilde{D}_i)  -
                  \left(\frac{1}{2}\sum_{i=1}^n \tilde{D}_i-\sum_{i\in\nodeM} \tilde{D}_i \right)
                \times    \sum_{i=1}^n   h(\zeta_i)}{    2\sum_{i=1}^n(
                h(\zeta_i) \hat{D}_i) \times  \sum_{i=1}^n h(\zeta_i)
                - {\left( 2\sum_{i=1}^n h(\zeta_i) \tilde{D}_i \right)}^2  }, \\
      \hat{a} &= -\frac{ \hat{b} \sum_{i=1}^n \left(h(\zeta_i)\tilde{D}_i \right) + \frac{n}{2}- \card{\nodeM}}{ \sum_{i=1}^n h(\zeta_i) }.
    \end{align}
  \item The parameters $\zeta$ maximizing $J_{\tau,\nu,\zeta,\theta,\psi}(\MAO,R)$  when  all  other parameters  are  held
    fixed are
    \begin{equation}\nonumber
      \hat{\zeta}_{i} = \sqrt{a^2 + b^2\hat{D}_i + 2ab\tilde{D}_i}, \ \forall i \in \mathcal{N}.
    \end{equation}
  \item The optimal  $\nu$ in $J_{\tau,\nu,\zeta,\theta,\psi}(\MAO,R)$
    when all other parameters are held fixed verify
    \begin{multline}
      \hat{\nu}_{ij}
      = \mathrm{logistic} \Bigg( \sum_{(q,\ell)\in\block^2} \tau_{iq}\tau_{j\ell} \log\left(\frac{\pi_{q\ell}}{1-\pi_{q\ell}}\right)
      - b \\
      + 2h(\zeta_i)\left( ab + b^2(1+\tilde{D}_{i}^{-j}) \right) + 2h(\zeta_j)\left( ab + b^2(1+\tilde{D}_{j}^{-i}) \right) \Bigg).
    \end{multline}
  \end{enumerate}
  Moreover, $\lambda_{iq} = 1 \ \forall (i,q) \in \node\times\block$
  for optimization of $\tau$ in Proposition~\ref{prop:nmar_common}.2).
\end{proposition}

\begin{proposition}
\label{prop:ICL_NMAR}
For  a model  with $Q$  blocks,  a sampling  design with  a vector  of
parameters        $\psi$        with       dimension       $K$        and
$(\hat{\theta},\hat{\psi})=\argmax_{(\theta,   \psi)}\log   p_{\theta,
  \psi}(\MAO,\MAM, R, Z)$, the ICL criterion is
\begin{equation}\nonumber
  \mathrm{ICL}        =       -2\mathbb{E}_{\tilde        p_{\tau,\nu};
    \hat{\theta},\hat{\psi}}\left[\log p_{\hat{\theta},\hat{\psi}}(\MAO,\MAM, R, Z | Q, K)\right] + \mathrm{pen}_{\text{ICL}},
\end{equation}
where
\begin{equation}\nonumber
  \mathrm{pen}_{\text{ICL}} = \left\{
    \begin{array}{ll}
      \left(K + \frac{Q(Q+1)}{2}\right)\log \left(\frac{n(n-1)}{2}\right) + (Q-1)\log (n) &  \text{if the sampling design} \\
                                                                                          & \text{is dyad-centered,}\\
      \frac{Q(Q+1)}{2}\log\left( \frac{n(n-1)}{2} \right)+ (K + Q-1)\log (n) & \text{otherwise.}\\
    \end{array}
  \right.
\end{equation}

\textcolor{red}{\begin{equation}\nonumber
  \mathrm{pen}_{\text{ICL}} = \left\{
    \begin{array}{ll}
      \left(K + Q^2\right)\log \left(n(n-1)\right) + (Q-1)\log (n) &  \text{if the sampling design} \\
                                                                                          & \text{is dyad-centered,}\\
      Q^2\log\left( n(n-1) \right)+ (K + Q-1)\log (n) & \text{otherwise.}\\
    \end{array}
  \right.
\end{equation}}
\end{proposition}

\section{Poisson SBM}
\subsection{Undirected/\textcolor{red}{Directed}}
\subsubsection{MAR}

% \noindent La borne inférieure de la vraisemblance des données observées a la forme suivante
% \begin{eqnarray*}
% \mathcal{J}_{partial}(\mathcal{R}_{\MA})&=& \text{log}(p_{\theta}(\MA_{obs})) - KL[\mathcal{R}_{\MA_{obs}}({Z}^{\prime})||p_{\theta}({Z}^{\prime}|\MA_{obs})], \\ 
% &=& \mathbb{E}_{\mathcal{R}_{\MA_{obs}}}[\text{log}(p_{\theta}(\MA_{obs},{Z}^{\prime}))]+\mathcal{H}(\mathcal{R}_{\MA_{obs}}({Z}^{\prime})),\\
% &=& \int \mathcal{R}_{\MA_{obs}}(Z)\text{log}\left( p_{\theta}(Z, \MA_{obs}) \right)dZ+\mathcal{H}\left( \mathcal{R}_{\MA_{obs}}(Z) \right),\\
% &=& \sum_{i}\sum_{q}\tau_{iq}\text{log}(\alpha_{q}) + \sum_{\MA_{ij}\in \MA_{obs}} \sum_{q, l=1}^{Q}\tau_{iq}\tau_{jl} \text{log} \{ p(\MA_{ij},\lambda_{ql}) \}  -\sum_{i}\sum_{q}\tau_{iq}\text{log}(\tau_{iq}).
% \end{eqnarray*}
% 
% Avec $\mathcal{R}_{\MA}(\cdot)=\mathbb{P}(\cdot|\MA)$ définit comme précédemment. En insistant sur le fait que $\mathcal{R}_{\MA}$ dépend des données $\MA$.\\
% 
% 
% \begin{prop}
% Étant donnés les paramètres $\alpha$ et $\lambda$, le paramètre variationnel optimal $\{\hat{\tau}_i\}=\underset{ \{\tau_i\}}{argmax \text{ } } \mathcal{J}(\mathcal{R}_{\MA_{obs}})$ satisfait la relation de point fixe suivante $$\hat{\tau}_{iq}\propto \alpha_{q} \left( \prod_{\MA_{ij}\in \MA_{obs}} \prod_{l}[p(\MA_{ij},\lambda_{ql})p(\MA_{ji},\lambda_{lq})]^{\hat{\tau}_{jl}}\right).$$
% \end{prop}
% 
% 
% \begin{prop}
% Étant donné les paramètres variationnels $\{\tau_i\}$, les valeurs des paramètres $\alpha$ et $\lambda$ qui maximisent $\mathcal{J}_{partial}(\mathcal{R}_{Y})$ sont $$\hat{\alpha}_q=\frac{1}{n}\sum_{i}\hat{\tau}_{iq}, \hspace{3mm} \hat{\lambda}_{ql}=\frac{\sum_{\MA_{ij}\in \MA_{obs}}\hat{\tau}_{iq}\hat{\tau}_{jl}\MA_{ij}}{\sum_{\MA_{ij}\in \MA_{obs}}\hat{\tau}_{iq}\hat{\tau}_{jl}}.$$
% \end{prop}
% 
% \textbf{Algorithme}\\
% \begin{enumerate}
% \item On commence par initialiser $\{{\tau_i}^{(0)}\}$
% \item Puis, itérativement, on met à jour les paramètres $\tau_i$, $\alpha_i$ et $\pi$ comme suit : 
% \begin{eqnarray*}
% \left({\alpha}^{(h+1)},{\lambda}^{(h+1)}\right)&=&\underset{(\alpha,\lambda)}{argmax \text{ } } \mathcal{J}_{partial}\left(\mathcal{R}_{\MA}; \ \{{\tau_i}^{(h)}\},\ \alpha,\ \lambda \right),\\
%  \{{\tau_i}^{(h+1)}\}&=&\underset{\{{\tau_i}\}}{argmax \text{ } } \mathcal{J}_{partial}\left(\mathcal{R}_{\MA}; \ \{{\tau_i}\},\ \alpha^{(h+1)},\ \lambda^{(h+1)} \right).\\
% \end{eqnarray*}
% \item Répéter 2. jusqu'à convergence  (typiquement si la vraisemblance n'augmente plus significativement d'une étape à une autre).\\
% \end{enumerate}
% 
% \begin{prop}
% Pour un nombre de classes $Q$ donné, l'algorithme décrit ci dessus génère une séquence $\{ \{{\tau_i}^{(h)}\},\alpha^{(h)}, \lambda^{(h)}\}_{h\geqslant 0}$ qui fait croitre $\mathcal{J}_{partial}(\mathcal{R}_{\MA})$ de tel sorte que 
% $$\mathcal{J}_{partial}\left(\mathcal{R}_{\MA}; \ \{{\tau_i}^{(h+1)}\},\ \alpha^{(h+1)},\ \lambda^{(h+1)} \right) \geqslant \mathcal{J}_{partial}\left(\mathcal{R}_{\MA}; \ \{{\tau_i}^{(h)}\},\ \alpha^{(h)},\ \lambda^{(h)} \right).$$
% \end{prop}


\textcolor{red}{In the directed case, only $\dyadO$ is changing in the following (i.e don't forget that the matrix is not symmetric: $\MA_{ij}$
and $\MA_{ji}$ are two different random variables).}
\begin{proposition}\label{prop:complete_loglik_sbm}      The      complete
  log-likelihood restricted to the observed variables is
  \begin{equation}
    \nonumber
    \log  p_{\theta}(\MAO,Z)=  \sum_{(i,j)\in  \dyadO}  \sum_{(q,\ell)
      \in\block^2}   Z_{iq}Z_{j\ell}\log   p(\MA_{ij},\lambda_{q\ell})   +
    \sum_{i\in \nodeO}\sum_{q\in\block} Z_{iq}\log (\alpha_{q}),
    % \label{loglik_obs_mar}
  \end{equation}
  with $p(x,\lambda)= \frac{\lambda^{x}}{x!}e^{-\lambda}$ the Poisson probability density function.
\end{proposition}

\begin{proposition} 
The variationnal approximation is
\begin{equation*}
  \label{eq:J_MAR}
  J_{\tau,\theta}(\MAO) =  \sum_{(i,j)\in  \dyadO}  \sum_{(q,\ell)\in\block^2}  \tau_{iq}\tau
  _{j\ell}\log       p(\MA_{ij},\lambda_{q\ell})      +       \sum_{i\in\nodeO}\sum_{q\in\block} \tau _{iq}\log (\alpha_{q}/\tau_{iq}).
\end{equation*}
\end{proposition}

\begin{proposition} Consider  the lower  bound $J_{\tau,\theta}(\MAO)$.
  \begin{enumerate}
  \item   The  parameters $\theta=(\alpha,  \lambda)$ maximizing  $J_\theta(\MAO)$
    when $\tau$ is held fixed are
    \begin{equation}\nonumber
      \hat{\alpha}_q=\frac{\sum_{i\in \nodeO}\hat{\tau}_{iq}}{\card{\nodeO}}, \qquad
      \hat{\lambda}_{q\ell}=\frac{\sum_{(i,j)\in
          \dyadO}\hat{\tau}_{iq}\hat{\tau}_{j\ell}\MA_{ij}}{\sum_{(i,j)\in
          \dyadO
        }\hat{\tau}_{iq}\hat{\tau}_{j\ell}}.
    \end{equation}
  \item   The  variational parameters  $\tau$  maximizing $J_\tau(\MAO)$  when
    $\theta$ is held fixed are obtained thanks to the following fixed point relation:
    \begin{equation}\nonumber
      \hat{\tau}_{iq}\propto \alpha_{q} \left( \prod_{(i,j)\in\dyadO}
        \prod_{\ell\in\block} p(\MA_{ij}; \lambda_{q\ell})^{\hat{\tau}_{j\ell}}\right).
    \end{equation}
    \textcolor{red}{    \begin{equation}\nonumber
      \hat{\tau}_{iq}\propto \alpha_{q} \left( \prod_{(i,j)\in\dyadO}
        \prod_{\ell\in\block} (p(\MA_{ij}; \lambda_{q\ell})p(\MA_{ji}; \lambda_{\ell q}))^{\hat{\tau}_{j\ell}}\right).
    \end{equation}}
  \end{enumerate}
\end{proposition}

\begin{proposition}
\label{prop:ICL_MAR}
  For      an      SBM      with     $Q$      blocks      and      for
  $\hat{\theta}=\argmax \log p_{\theta}(\MAO,Z)$, the ICL criterion is
  given by
  \begin{equation}\nonumber
    \mathrm{ICL}(Q)   =    -2   \mathbb{E}_{\tilde   p_\tau}\left[\log
      p_{\hat\theta}(\MAO,  Z   ;  Q)\right]   +  \frac{Q(Q+1)}{2}\log
    \card{\dyadO} + (Q-1)\log \card{\nodeO}. 
  \end{equation}
  \textcolor{red}{  \begin{equation}\nonumber
    \mathrm{ICL}(Q)   =    -2   \mathbb{E}_{\tilde   p_\tau}\left[\log
      p_{\hat\theta}(\MAO,  Z   ;  Q)\right]   +  Q^2\log
    \card{\dyadO} + (Q-1)\log \card{\nodeO}. 
  \end{equation}}
\end{proposition}



\end{document}
