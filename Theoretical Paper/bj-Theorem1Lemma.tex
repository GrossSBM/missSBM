\section{Technical Lemma}
\label{sec:technical-lemma}


We start with the proof of Lemma~\ref{cor:prob-regular-configurations-star}. 

\proofbegin
Each $\zsumk$ is a sum of $\n$ i.i.d Bernoulli r.v. with parameter $\pik \geq \pii_{\min} \geq c$. A simple Hoeffding bound shows that
\begin{equation*}
  \Prob_{\bthetas}\left( \zsumk \leq \n \frac{c}{2} \right)
  \leq
  \Prob_{\bthetas}\left( \zsumk \leq \n \frac{\pik}{2} \right)
  \leq
  \exp\left( - 2\n\left(\frac{\pik}{2}\right)^2 \right)
  \leq
  \exp\left( - \frac{\n c^2}{2} \right)
\end{equation*}
Similarly,
\begin{equation*}
  \Prob_{\bthetas}\left( \wsuml \leq \dd \frac{c}{2} \right) \leq \exp\left( - \frac{\dd c^2}{2} \right)
\end{equation*}
The proposition follows from a union bound over $\g$ values of $\kk$ and $\m$ values of $\el$.
\proofend

%% Subexponential variables
We now prove two propositions regarding subexponential variables. 

\begin{proposition}\label{proposition:maxzw}\textbf{Maximum in $(\bz, \bw)$}\\
  Let $(\bz, \bw)$ be a configuration and $\hat{x}_{\kk, \el}(\bz, \bw)$ resp. $\barxkl(\bz, \bw)$ be as defined in Equations~\eqref{eq:mle-complete-likelihood} and~\eqref{eq:profile-likelihood-notations}. Under the assumptions of the section~\ref{sec:assumptions}, for all $\vareps>0$
  \begin{equation}
    \label{eq:ineg-de-bernst-1}
    \Prob \left(\max_{\bz, \bw} \max_{k,l} \pizk \rhowl |\hat{x}_{\kk, \el} - \barxkl| > \vareps \right)  \leq  \g^{\n+1} \m^{\dd + 1} \exp \left( - \frac{nd\vareps^2}{2(\bar{\sigma}^2 + \neighborsize^{-1}\vareps)}\right). \\
  \end{equation}
  Additionally, the suprema over regular assignments satisfies for all $c\in\left]0;\min(\pii_{\min},\rhoo_{\min}) / 2\right]$:
  \begin{equation}
    \label{eq:ineg-de-bernst-2}
    \Prob \left(\max_{\bz \in \mcZ_1, \bw \in \mcW_1} \max_{k,l} |\hat{x}_{\kk, \el} - \barxkl| > \vareps \right) \leq  \g^{\n+1} \m^{\dd + 1} \exp \left( - \frac{ndc^2\vareps^2}{8(\bar{\sigma}^2 + \neighborsize^{-1}\vareps)}\right). \\
  \end{equation}
\end{proposition}

\proofbegin

The random variables $X_{ij}$ are subexponential with parameters $(\vmax^2, 1/\neighborsize)$. Conditionally to $(\bzs, \bws)$, $\zsumk \wsuml(\hat{x}_{\kk, \el} - \barxkl)$ is a sum of $\zsumk \wsuml$ centered subexponential random variables. By Bernstein's inequality, we therefore have for all $x \leq \neighborsize$
\begin{align*}
\Prob( \zsumk \wsuml | \hat{x}_{\kk, \el} - \barxkl| \geq x ) & \leq \exp\left( - \frac{x^2}{2(\zsumk \wsuml\bar{\sigma}^2 + \neighborsize^{-1} x)}\right) \\
\end{align*}
In particular, if $x \leq \neighborsize / \n\dd$,
\begin{align*}
\Prob\left( \pizk \rhowl | \hat{x}_{\kk, \el} - \barxkl| \geq x \right) & \leq \exp\left( - \frac{ndx^2}{2(\pizk\rhowl\bar{\sigma}^2 + \neighborsize^{-1} x)}\right) \leq \exp\left( - \frac{ndx^2}{2(\bar{\sigma}^2 + \neighborsize^{-1} x )}\right) \\
\end{align*}
uniformly over $(\bz, \bw)$. Equation~\eqref{eq:ineg-de-bernst-1} then results from a union bound.
Similarly,
\begin{align*}
  \Prob\left( | \hat{x}_{\kk, \el} - \barxkl| \geq x \right) & = \Prob\left( \pizk \rhowl | \hat{x}_{\kk, \el} - \barxkl| \geq \pizk \rhowl x \right) \\
  & \leq \exp\left( - \frac{ndx^2\pizk^2\rhowl^2}{2(\pizk\rhowl\bar{\sigma}^2 + \neighborsize^{-1} x \pizk\rhowl)}\right) \\
  & \leq \exp\left( - \frac{ndc^2x^2}{8(\bar{\sigma}^2 + \neighborsize^{-1} x )}\right)
\end{align*}
Where the last inequality comes the fact that regular assignments satisfy $\pizk \rhowl \geq c^2/4$. Equation~\eqref{eq:ineg-de-bernst-2} then results from a union bound over $\mcZ_1 \times \mcW_1 \subset \mcZ \times \mcW$.
\proofend

\begin{proposition}[concentration for subexponential]
\label{prop:concentration-subexponential}
Let $\X_{1}, \dots, \X_{\n}$ be independent zero mean random variables, subexponential with parameters $(\sigma_i^2, b_i)$. Note $V_0^2  = \sum_{\ii} \sigma_i^2$ and $b = \max_{i} b_i$. Then the random variable $Z$ defined by:
\begin{equation*}
  Z = \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}} \sum_{\ii} \Gamma_i X_{\ii}
\end{equation*}
is also subexponential with parameters $(M^2 V_0^2 + \n \log(2), Mb)$ and the following Bernstein inequality holds:
\begin{equation}
  \label{eq:concentration-subexponential}
  \Prob( Z - \Esp[Z] \geq t) \leq 
  \begin{cases} 
    \exp\left( - \frac{t^2}{2 M^2 V_0^2 \log(2)}\right)  & \text{if} \quad 0 \leq t \leq \frac{M^2 V_0^2 + \n \log(2)}{Mb} \\ 
    \exp\left( - \frac{t^2}{2Mb}\right)  & \text{if} \quad t \geq \frac{M^2 V_0^2 + \n \log(2)}{Mb}
  \end{cases}
\end{equation}
Moreover $\Esp[Z] \leq M V_0 \sqrt{\n}$. 
\end{proposition}

\proofbegin
We simply need to bound $\Esp[Z]$ and to show that $Z$ is sub-exponential. Equation~\eqref{eq:concentration-subexponential} results from properties of sub-exponential random variables. Choose $\lambda \in \R$ such that $|\lambda| \leq 1/Mb$: 
\begin{align*}
\Esp[\exp(\lambda Z)] & = \Esp\left[ \exp\left( \lambda \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}} \sum_{\ii} \Gamma_{\ii} X_{\ii} \right)\right] \leq \Esp\left[ \prod_{\ii} \exp\left( \lambda \sup_{| \Gamma_{\ii} | \leq M}  \Gamma_{\ii} X_{\ii} \right)\right] \\ & \leq \prod_{\ii}  \Esp\left[  \exp\left( \lambda M X_{\ii} \right) + \exp\left( -\lambda M X_{\ii} \right)\right] \leq \prod_{\ii}  2\exp\left( \frac{M^2 \lambda^2 \sigma_{\ii}^2}{2}\right) = \exp\left( \frac{M^2 \lambda^2 V_0^2 + \n \log(2)}{2}\right)
\end{align*}
where the last inequality comes from the sub-exponential nature of the $X_{\ii}$. This proves that $Z$ is sub-exponential with parameters $(M^2 V_0^2 + \n \log(2), Mb)$. We now bound $\Esp[Z]$. 
\begin{align*}
  \Esp[Z] & = \Esp \left[ \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}}  \sum_{\ii} \Gamma_{\ii} X_{\ii} \right] = \Esp \left[ \sum_{\ii} M |X_{\ii}| \right] \leq M \sum_{\ii} \sqrt{[ X_{\ii}^2 ]} \\ & 
  = M \sum_{\ii} \sigma_i \leq M \left( \sum_{\ii} 1 \right)^{1/2} \left( \sum_{\ii} \sigma_{\ii}^2 \right)^{1/2} = M V_0\sqrt{\n}
\end{align*}
where the first inequality is Jensen's and the second one is Cauchy-Swcharz. 

\proofend

%% Lemma on symmetric configurations
The next lemma deals with the contributions of symmetric and equivalent configurations to the observed likelihood. 

\begin{lemme}
  \label{lem:symmetry}
  For all $\btheta \in \bTheta$, Consider the function 
  \begin{equation*}
    H_{\n\dd}(\btheta) = \n \KL(\bpis, \bpi) + \dd \KL(\brhos, \brho) + \n\dd \sum_{\kk,\el} \pik^\vrai \rhol^\vrai \KL(\alskl, \alkl')
  \end{equation*}
  Then
  \begin{equation*}
    \sum_{(\bz, \bw) \sim (\bzs, \bws)} \prob(\bx, \bz, \bw; \btheta) = \# \Symmetric(\btheta) \# \argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta') \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') (1 + \smallO_P(1))
  \end{equation*}
  where the $\smallO_P$ is uniform in $\btheta$. And specifically for $\bthetas$, 
  \begin{equation*}
    \sum_{(\bz, \bw) \sim (\bzs, \bws)} \prob(\bx, \bz, \bw; \bthetas) = \# \Symmetric(\bthetas) \prob(\bx, \bzs, \bws; \bthetas) (1 + \smallO_P(1))
  \end{equation*}
\end{lemme}

\proofbegin
Choose $(s, t)$ permutations of $\{1, \dots, \g\}$ and $\{1, \dots, \m\}$ and assume that $\bz = \bz^{\vrai,s}$ and $\bw = \bw^{\vrai, t}$. Then $\prob(\bx, \bz, \bw; \btheta) = \prob(\bx, \bz^{\vrai, s}, \bw^{\vrai, t}; \btheta) = \prob(\bx, \bzs, \bws; \btheta^{s,t})$.  If furthermore $(s, t) \in \Symmetric(\btheta)$, $\btheta^{s, t} = \btheta$ and immediately $\prob(\bx, \bz, \bw; \btheta) = \prob(\bx, \bzs, \bws; \btheta)$. We can therefore partition the sum as 

\begin{align*}
  \sum_{(\bz, \bw) \sim (\bz, \bw)} \prob(\bx, \bz, \bw; \btheta) & = \sum_{s, t} \prob(\bx, \bz^{\vrai, s}, \bw^{\vrai, t}; \btheta) \\ 
  & = \sum_{s, t} \prob(\bx, \bzs, \bws; \btheta^{s,t}) \\ 
  & = \sum_{\btheta' \sim \btheta} \# \Symmetric(\btheta') \prob(\bx, \bzs, \bws; \btheta') \\ 
  & = \# \Symmetric(\btheta) \sum_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') \\ 
\end{align*}
We can easily show that 
\begin{equation*}
  \log \frac{\prob(\bx, \bzs, \bws; \btheta)}{\prob(\bx, \bzs, \bws; \bthetas)} = - H_{\n\dd}(\btheta) ( 1 + \smallO_P(1))
\end{equation*}
where the $\smallO_P$ is uniform over $\bTheta$. For $\btheta$ fixed, since there are at most $\g^\g \m^\m$ $\btheta'$ that are equivalent to $\btheta$, $\prob(\bx, \bzs, \bws; \btheta')$ reaches its maximum on an element of $\argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta')$ with high probability for $\n$ and $\dd$ large enough. 

Assume without loss of generality that $\btheta \in \argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta')$ and realizes the maximum of $\prob(\bx, \bzs, \bws; \btheta')$ over $\btheta' \sim \btheta$. Choose $\btheta'$ not in the $\argmax$. 
\begin{align*}
\log \frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \btheta)} & = \left( H_{\n\dd}(\btheta) - H_{\n\dd}(\btheta') \right) (1 + \smallO_P(1)) \xrightarrow[\n,\dd \to +\infty]{} -\infty
\end{align*}
so that 
\begin{align*}
  \sum_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') = \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') \# \argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta') \left(1 + \smallO_P(1) \right)
\end{align*}
which proves the first equation. The second comes from the fact that 
\begin{equation*}
  \argmax_{\btheta \sim \bthetas} H_{\n\dd}(\btheta) = \{ \bthetas \} \quad \text{and} \quad \Prob_{\bthetas}\left( \max_{\btheta \sim \bthetas} \prob(\bx, \bzs, \bws; \btheta) \neq \prob(\bx, \bzs, \bws; \bthetas) \right) = \smallO(1)
\end{equation*}
\proofend

The final lemma allows is the working horse for proving Proposition~\ref{prop:maximum-conditional-likelihood}. 

%\begin{lemme}\label{lemme:casdegalite}\textbf{}\\
%
%\textcolor{blue}{[In french] Soit $\etaa$ et $\bar{\etaa}$ deux matrice de $M_{\g \times \m}(\Theta)$ et $f: \Theta \times \Theta \to \mathbb{R_+}$ une fonction à valeurs positives, $A$ une matrice de confusion (carrée) de taille $\g$ et $B$ une matrice de confusion de taille $\m$. On note $D_{\kk\el\kk'\el'} = f(\etaa_{\kk\el}, \bar{\etaa}_{\kk'\el'})$. On suppose que
%\begin{itemize}
%\item toutes les lignes de $\etaa$ sont distinctes;
%\item toutes les colonnes de $\etaa$ sont distinctes;
%\item $f(x,y) = 0 \Leftrightarrow x = y$;
%\item toutes les lignes de $A$ contiennent un élément non nul;
%\item toutes les lignes de $B$ contiennent un élément non nul;
%\end{itemize}
%et on note
%\begin{equation}
%\label{eq:casdegalite}
%\Sigma = \sum_{\kk \kk'} \sum_{\el \el'} A_{\kk \kk'} B_{\el \el'} d_{\kk\el\kk'\el'}
%\end{equation}
%Alors
%\begin{equation*}
%  \Sigma = 0 \Leftrightarrow \begin{cases}
%    A, B \text{ sont les matrices des permutations } s,t & \\
%    \bar{\etaa} = \etaa^{s, t} \text{ cad } \forall (\kk, \el), \bar{\etaa}_{\kk\el} = \etaa_{s(\kk) t(\el)} &
%  \end{cases}
%\end{equation*}}
%\end{lemme}
%
%\proofbegin
%Supposons que $A$ et $B$ sont des matrices de permutations, correspondant aux permutations $s$ et $t$, c'est à dire $A_{ij} = 0$ si $i \neq s(j)$ et $B_{ij} = 0$ si $i \neq t(j)$. Comme chaque ligne de $A$ contient un élément non nul, on sait de plus que $A_{s(\kk)\kk} > 0$ (resp $B_{s(\el)\el} > 0$) pour tout $\kk$ (resp. $\el$). La somme $\Sigma$ se réduit donc à
%\begin{equation*}
%  \Sigma = \sum_{\kk \kk'} \sum_{\el \el'} A_{\kk \kk'} B_{\el \el'} d_{\kk\el\kk'\el'} = \sum_{\kk} \sum_{\el} A_{s(\kk)\kk} B_{t(\el)\el} d_{s(\kk)t(\el) \kk\el}
%\end{equation*}
%$\Sigma$ est une somme nulle de produits de termes positifs, chaque produit est donc nul. Les termes $A_{s(\kk)\kk}$ et $B_{t(\el)\el}$ sont tous non nuls, on en déduit que pour tout $(\kk, \el)$, $d_{s(\kk)t(\el) \kk\el} = 0$ et donc que $\bar{\etaa}_{\kk\el} = \etaa_{s(\kk) t(\el)}$.
%
%Raisonnons par l'absurde en supposant que $A$ ou $B$ ne sont pas des matrices de permutations et que $\Sigma = 0$. Sans perte de généralité, on suppose que $A$ n'est pas une matrice de permutation. Il existe donc une colonne $k$ de $A$ qui contient $2$ éléments non nuls: $A_{\kk_1 \kk}$ et $A_{\kk_2 \kk}$. Soit $\el \in \{1\dots\m\}$, il existe par hypothèse un $\el'$ tel que $B_{\el \el'} \neq 0$. Comme $\Sigma$ est nulle, les deux produits $A_{\kk_1 \kk} B_{\el \el'} d_{\kk_1 \el \kk \el'}$ et $A_{\kk_2 \kk} B_{\el \el'} d_{\kk_2 \el \kk \el'}$ le sont aussi.
%\begin{equation*}
%  \begin{cases}
%    A_{\kk_1 \kk} B_{\el \el'} d_{\kk_1 \el \kk \el'} & = 0 \\
%    A_{\kk_2 \kk} B_{\el \el'} d_{\kk_2 \el \kk \el'} & = 0 \\
%  \end{cases}
%  \Leftrightarrow
%  \begin{cases}
%    d_{\kk_1 \el \kk \el'} & = 0 \\
%    d_{\kk_2 \el \kk \el'} & = 0 \\
%  \end{cases}
%  \Leftrightarrow
%  \begin{cases}
%    \etaa_{\kk_1 \el} = \bar{\etaa}_{\kk \el'} & \\
%    \etaa_{\kk_2 \el} = \bar{\etaa}_{\kk \el'} & \\
%  \end{cases}
%  \Leftrightarrow
%  \etaa_{\kk_1 \el} = \etaa_{\kk_2 \el}
%\end{equation*}
%L'égalité précédente est vraie pour tout $\el$ donc les lignes $\kk_1$ et $\kk_2$ de $\etaa$ sont identiques, ce qui contredit les hypotèses de l'énoncé.
%
%La réciproque est immédiate.
%\proofend
