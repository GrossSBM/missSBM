\section{Proofs}
\label{sec:proofPL}
%\subsection{Proof of Proposition~\ref{prop:LocalAsymp}} 



\subsection{Proof of Proposition~\ref{prop:profile-likelihood}} 
\proofbegin
Define
    $\nu(y, \al)  = y \al - \norm (\al)$. For $\y$ fixed, $\nu(y, \al)$ is maximized at $\al = \normpm(y)$. Define $n_{q\el}(z)=\left( \sum_{i<j}\rrij\ziq\zjl \right)$, manipulations yield
\begin{align*}
  &\Fn(\bal, \bz)= \log \prob(\byo; \bz,\btheta) - \log {\prob(\byo; \bzs,\bthetas)} \\
%   & = \n\dd \left[ \sum_{\kk} \sum_{\el} \pizk \rhowl \left\{\hxklzw \alkl - \norm(\alkl) \right\}  - \sum_{\kk} \sum_{\el} \pizsk \rhowsl \left\{\hxkl(\bzs, \bws) \alskl - \norm(\alskl) \right\} \right] \\
  & = \left[ \sum_{\kk} \sum_{\el} n_{q\el}(z) \nu(\hyklz, \alkl)  - \sum_{\kk} \sum_{\el} n_{q\el}(z^\star)\nu(\hykl(\bzs), \alskl) \right]
\end{align*}
which is maximized at $\alkl = \normpm(\hyklz)$. Similarly
\begin{align*}
 & \G(\bal, \bz)  = \Esp_{\bthetas} [ \log \prob(\byo; \bz,\btheta) - \log \prob(\byo; \bzs,\bthetas) ] \\
  & = \left[ \sum_{\kk} \sum_{\el} n_{q\el}(z) \nu(\baryklz, \alkl)  - \sum_{\kk} \sum_{\el} n_{q\el}(z^\star) \nu(\normp(\alskl), \alskl) \right]
\end{align*}
is maximized at $\alkl = \normpm(\baryklz)$
\proofend

\subsection{Proof of Proposition~\ref{prop:maximum-conditional-likelihood}  (maximum of $\G$ and $\Lambtilde$)} 

\proofbegin
We condition on $\bzs$ and prove Equation~\eqref{eq:conditional-likelihood-second-form}:
\begin{align*}
  \G(\btheta, \bz) & = \Esp_{\bthetas} \left[ \left. \frac{\prob(\byo; \bz,\btheta)}{\prob(\byo; \bzs,\bthetas)} \right| \bzs \right] \\
  & = \sum_{\ii} \sum_{\jj} \sum_{\kk,\kp}\ \sum_{\el,\lp} \Esp_{\bthetas}\left[ y_{ij}  (\al_{\kp\lp} - \alskl) - (\norm(\al_{\kp\lp}) - \norm(\alskl)) \right] \rho \z^\vrai_{\ii\kk} \z_{\ii\kp} \z^\vrai_{\jj\el} \z_{\jj\lp} \\
  & = \n^2 \rho \sum_{\kk, \kp} \sum_{\el, \lp}  \RQbz_{\kk,\kp} \RQbz_{\el, \lp} \left[ \normp(\alskl) (\al_{\kp\lp} - \alskl) +  \norm(\alskl) - \norm(\al_{\kp\lp}) \right] \\
  & = - \n^2 \rho \sum_{\kk,\kp} \sum_{\el,\lp} \RQbz_{\kk,\kp} \RQbz_{\el, \lp} \KL(\al^\vrai_{\kk\el}, \al_{\kp\lp})
\end{align*}

If $\bzs$ is regular, and for $n > 2/c$, all the rows of $\RQbz$ have
at least one positive element and we can apply
lemma~\ref{lemme:casdegalite} (which is an adaptation for LBM of Lemma 3.2 of \cite{bickel2013asymptotic} for SBM) to characterize the maximum for
$\G$.

The maximality  of $\Lambtilde(\bzs)$ results from the fact that $\Lambtilde(\bz) = \G(\bar{\bal}(\bz), \bz)$ where
$\bar{\bal}(\bz)$ is a particular value of $\bal$, $\Lambtilde$ is
immediately maximum at $\bz \sim \bzs$, and for those, we have $\bar{\bal}(\bz) \sim \bals$.

The separation and local behavior of $G$ around $\bzs$ is a direct consequence of the proposition \ref{prop:profile-likelihood-derivative}.

\proofend

\subsection{Proof of Proposition~\ref{prop:profile-likelihood-derivative} (Local upper bound for  $\Lambtilde$)}

% \proofbegin
% We work conditionally on $(\bzs, \bws)$. Note that $\Lambtilde$ only depends on $\bz$ through $\Rgbz$ and $\Rmbw$. We can therefore extend it to matrices $(U, V) \in \mathcal{U} \times\mathcal{V}$ where $\mathcal{U} = \{U \in [0, 1]^{\g \times \g}: U \un = \bpi(\bzs)\}$ and $\mathcal{V} = \{V \in [0, 1]^{\m \times \m}: V \un = \brho(\bws)\}$ with $\un=(1,\ldots, 1)\transpose$ a vector only containing $1$ values.
% \[
%   \Lambtilde(U, V)  = - \n\dd \sum_{\kk,\kp} \sum_{\el,\lp} U_{\kk\kp} V_{\el\lp} \KL\left( \alskl , \baral_{\kp\lp} \right)
%   \] where
% \[ \baralkl = \baralkl(U, V) = \normpm \left( \frac{\left[U\transpose \Sal V \right]_{\kk\el}}{\sum_{\kp} U_{\kp\kk} \sum_{\lp} V_{\lp\el}} \right) 
% \]
% 
% The maps $\al \mapsto \KL(\al, \alskl)$ are all twice differentiable with second derivative bounded by $\vmax^2$ on $[-M_{\al}, M_{\al}]$ and the maps $(U, V) \mapsto \baralkl(U, V)$ map $\mathcal{U} \times \mathcal{V}$ to $[-M_{\al}, M_{\al}]$. All the second order partial derivatives of $\Lambtilde$ are therefore continuous in $(U, V)$ and bounded on $\mathcal{U} \times \mathcal{V}$. We abbreviate  $(D_{\pii}, D_{\rhoo}) \coloneqq (\Diag(\bpi(\bzs)), \Diag(\brho(\bws)))$ and note for $(U, V) \in \mathcal{U} \times \mathcal{V}$.
% \begin{equation*}
%   g_{U, V}: t \in [0, 1] \mapsto \frac{1}{\n\dd}\Lambtilde\left[ (1-t)D_{\pii} + tU, (1-t)D_{\rhoo} + tV \right]
% \end{equation*}
% % It comes from the boundedness of the second partial derivatives that for all $\vareps >0$, for $t$ small enough
% % \begin{equation*}
% %   g_{U, V}'(t) - g_{U, V}'(0) \leq \vareps
% % \end{equation*}
% % for all $(U, V) \in \mathcal{U} \times \mathcal{V}$.
% We now give an upper bound of $g_{U, V}'(0)$. Tedious but straightforward computations show that:
% \begin{align*}
%   g_{U, V}'(0) & = - \sum_{\el} \sum_{\kk, \kp} D_{\rhoo, \el\el} (U_{\kk\kp} - D_{\pii,\kk\kp}) \KL(\alskl, \als_{\kp\el}) - \sum_{\kk} \sum_{\el, \lp} D_{\pii, \kk\kk} (V_{\el\lp} - D_{\rhoo,\el\lp}) \KL(\alskl, \als_{\kk\lp}) \\
%   & = - \sum_{\kk \neq \kp} U_{\kk\kp} \sum_{\el} D_{\rhoo, \el\el} \KL(\alskl, \als_{\kp\el}) - \sum_{\el \neq \lp} V_{\el\lp} \sum_{\kk} D_{\pii, \kk\kk} \KL(\alskl, \als_{\kk\lp}) \\
%   & \leq -\frac{c}{2} \sum_{\kk \neq \kp} U_{\kk\kp} \min_{\kk, \kp} \max_{\el} \KL(\alskl, \als_{\kp\el}) -\frac{c}{2} \sum_{\el \neq \lp} V_{\el\lp} \min_{\el, \lp} \max_{\kk} \KL(\alskl, \als_{\kk\lp}) \\
%   & \leq -\frac{c\delta(\bals)}{2} \left[ (1 - \Trace(U)) + (1 - \Trace(V)) \right] \\
%   & = - \frac{c \delta(\bals)}{2} \left[ \Trace(D_{\pii} - U) + \Trace(D_{\rhoo} - V) \right]
% \end{align*}
% where the second line comes from $\KL(\alskl, \alskl)=0$, the third from $D_{\rhoo, \el\el} \geq c/2$ and $D_{\pii, \kk\kk} \geq c/2$ for regular $(\bzs, \bws)$, the fourth from the definition of $\delta(\bals)$ and the fact that $\un\transpose U \un = \un \transpose V \un = 1$ and the last from $\Trace(D_{\pii}) = \Trace(D_{\rhoo}) = 1$. Since $\Lambtilde(D_{\pii}, D_{\rhoo}) = 0$, it comes from the boundedness of the second order partial derivatives of $\Lambtilde$ that for $c_3$ small enough, for all $(U, V)$ in $B(D_{\pii}, D_{\rhoo}, c_3) = \{(U, V) \in \mathcal{U} \times \mathcal{V} : \max(\| U - D_{\pii} \|_1 , \| V - D_{\rhoo} \|_1) \leq c_3 \}$:
% \begin{equation*}
%   \frac{\Lambtilde\left(U, V \right)}{\n\dd} \leq - \frac{c \delta(\bals)}{4} \left[ \Trace(D_{\pii} - U) + \Trace(D_{\rhoo} - V) \right]
% \end{equation*}
% \CK{Je n'ai pas compris. $\Lambtilde$ est lipschizienne donc ses derivees sont bornees, $|(g(0)-g(1))/(1-0)|=|g(1)|<K$, $g'(0)<-B<0$ donc $K> B$ mais comment arriver Ã  l'inequation ci-dessus?}
% Choose $(\bz,\bw)$ such that $\| \bz - \bzs\|_{0} \leq c_3 \n /2$ and $\| \bw - \bws\|_{0} \leq c_3 \dd /2$. Note that $\Rgbz \in\mathcal{U}$ and $\| \Rgbz - D_{\pii} \|_1 = 2 \Trace(D_{\pii} - \Rgbz) = 2 \| \bz - \bzs\|_{0} / \n \leq c_3$. Similarly, $\Rmbw \in \mathcal{V}$ and $\| \Rmbw - D_{\rhoo} \|_1 \leq c_3$. Therefore
% \begin{align*}
%   \Lambtilde\left(\bz, \bw \right) & \leq - \n\dd \frac{c \delta(\bals)}{4} \left[ \Trace(D_{\pii} - \Rgbz) + \Trace(D_{\rhoo} - \Rmbw) \right] \\
%   & = - \frac{c \delta(\bals)}{4}\left( \dd \|\bz - \bzs\|_0 + \n \|\bw - \bws\|_0 \right).
% \end{align*}
% \proofend
% 
% \subsection{Proof of Proposition~\ref{prop:profile-likelihood-convergence-local} (local convergence $\Fnd$)} 
% 
% \proofbegin
% We work conditionally on $(\bzs, \bws) \in \mcZ_1 \times \mcW_1$. Choose $\vareps \leq \neighborsize \vmax^2$ small and $\tilde{c} \leq c/4$. Assignments $(\bz, \bw)$ at hamming-distance less than $\tilde{c}$ of $(\bzs, \bws)$ are regular as defined in definition~\ref{def:regular} but with constant $\tilde{c}$. According to Proposition~\ref{proposition:maxzw}, $\hxkl$ and $\barxkl$ are at distance at most $\vareps$ with probability higher than $1 - \exp \left( - \frac{\n\dd \tilde{c}^2 \vareps^2}{8(\vmax^2 + \neighborsize^{-1} \vareps)}\right)$. Manipulation of $\Lamb$ and $\Lambtilde$ yield
% \begin{align*}
%   \frac{\Fnd(\btheta, \bz, \bw) - \Lambtilde(\bz, \bw)}{\n\dd} & \leq \frac{\Lamb(\bz, \bw) - \Lambtilde(\bz, \bw)}{\n\dd} \\
%   & = \sum_{\kk, \kp'} \sum_{\el, \lp} \Rgbz_{\kk\kp} \Rmbw_{\el\lp} \left[ f_{\kk\el}(\hx_{\kp\lp}) - f_{\kk\el}(\barx_{\kp\lp}) \right]
% \end{align*}
% where $f_{\kk\el}(x) = - \Salkl \normpm(x) + \norm\circ\normpm(x)$. The functions $f_{\kk\el}$ are twice differentiable on $\mathring{\mathcal{A}}$ with bounded first and second derivatives over $I = \normp([-M_{\al} - \neighborsize, M_{\al} + \neighborsize])$ so that:
% \begin{equation*}
%   f_{\kk\el}(y) - f_{\kk\el}(x) = f'_{\kk\el}(x) \left( y - x \right) + \smallO\left( y - x \right)
% \end{equation*}
% where the $\smallO$ is uniform over pairs $(x,y) \in I^2$ at distance less than $\vareps$ and does not depend on $(\bzs, \bws)$. $\barxkl$ is a convex combination of the $\Salkl = \normp(\alskl)$. Since $\normp$ is monotonic, $\barxkl \in \normp([-M_{\al}, M_{\al}]) \subset I$. Similarly, $| \hxkl - \barxkl | \leq \neighborsize \vmax^2$ and $|\normp'| \leq \vmax^2$ over $I$ therefore $\hxkl \in I$. We now bound $f'_{\kk\el}$:
% \begin{align*}
%   | f'_{\kk\el}(\barx_{\kp\lp}) | &= \left| \frac{\barx_{\kp\lp} - \Salkl}{\normp' \circ \normpm (\barx_{\kp\lp})} \right|  = \left| \frac{\frac{\left[ \Rgbz\transpose \Sal \Rmbw \right]_{\kp\lp}}{\pizk\rhowl} - \Salkl}{\normp' \circ \normpm (\barx_{\kp\lp})} \right| \\
%   & \leq \left( 1 - \frac{\Rgbz_{\kk\kp}\Rmbw_{\el\lp}}{\pizk\rhowl}\right) \frac{\Salmax - \Salmin}{\vmin^2}
% \end{align*}
% where $\Salmax = \max_{\kk,\el} \Salkl$ and $\Salmin = \min_{\kk,\el} \Salkl$. In particular,
% \begin{align*}
%   \Rgbz_{\kk\kp}\Rmbw_{\el\lp} | f'_{\kk\el}(\barx_{\kp\lp}) | & \leq \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \left( 1 - \frac{\Rgbz_{\kk\kp}\Rmbw_{\el\lp}}{\pizk\rhowl}\right) \frac{\Salmax - \Salmin}{\vmin^2}  \\
%   & \leq
%   \begin{cases}
%     \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \frac{\Salmax - \Salmin}{\vmin^2} & \text{if } (\kp, \lp) \neq (\kk, \el) \\
%     \left[ \pizk \rhowl - \Rgbz_{\kk\kk}\Rmbw_{\el\el} \right]  \frac{\Salmax - \Salmin}{\vmin^2} & \text{if } (\kk, \el) = (\kk, \el)
%   \end{cases}
% \end{align*}
% Wrapping everything,
% \begin{align*}
% \frac{| \Lamb(\bz, \bw) - \Lambtilde(\bz, \bw) | }{\n\dd} & = \left| \sum_{\kk,\kp} \sum_{\el,\lp} \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \left[ f'_{\kk\el}(\barx_{\kp\lp}) (\hxkl -\barxkl) + \smallO(\hxkl -\barxkl) \right]\right| \\
% & \leq \left[ \sum_{(\kp,\lp) \neq (\kk, \el)} \Rgbz_{\kk\kp}\Rmbw_{\el\lp} + \sum_{\kk, \el} ( \pizk \rhowl - \Rgbz_{\kk\kk}\Rmbw_{\el\el} ) \right]\\ & \times \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
% & = 2 \left[ \sum_{(\kp,\lp) \neq (\kk, \el)} \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \right] \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
% & = 2 \left[ 1 - \Trace(\Rgbz)\Trace(\Rmbw) \right] \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
% & \leq 2\left( \frac{\|\bz -\bzs\|}{\n} + \frac{\|\bw -\bws\|}{\dd} \right) \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
% & \leq 2\left( \frac{\|\bz -\bzs\|}{\n} + \frac{\|\bw -\bws\|}{\dd} \right) \frac{\Salmax - \Salmin}{\vmin^2} \vareps (1 +\smallO(1))
% \end{align*}
% We can remove the conditioning on $(\bzs, \bws)$ to prove Equation~\eqref{eq:profile-likelihood-convergence-local} with $c_2 = 2(\Salmax - \Salmin) / \vmin^2$ and $c_1 = c_2^2$.
% \proofend

\proofbegin

We work conditionally on $(\bzs, \bws)$. The principle of the proof relies on the extension of $\Lambtilde$ to a continuous subspace of$ M_g([0, 1]) \times M_m([0, 1])$, in which confusion matrices are naturally embedded. The regularity assumption allows us to work on a subspace that is bounded away from the borders of $M_g([0, 1]) \times M_m([0, 1])$. The proof then proceeds by computing the gradient of $\Lambtilde$ at and around its argmax and using those gradients to control the local behavior of $\Lambtilde$ around its argmax. The local behavior allows us in turn to show that $\Lambtilde$ is well-separated. 


 Note that $\Lambtilde$ only depends on $\bz$ and $\bw$ through $\Rgbz$ and $\Rmbw$. We can therefore extend it to matrices $(U, V) \in \mathcal{U}_c \times\mathcal{V}_c$ where $\mathcal{U}$ is the subset of matrices $\mathcal{M}_{\g}([0, 1])$ with each row sum higher than $c/2$ and $\mathcal{V}$ is a similar subset of $\mathcal{M}_{\m}([0, 1])$. 
\[
  \Lambtilde(U, V)  = - \n\dd \sum_{\kk,\kp} \sum_{\el,\lp} U_{\kk\kp} V_{\el\lp} \KL\left(\alskl, \baral_{\kp\lp} \right)
  \] where
\[ \baralkl = \baralkl(U, V) = \normpm \left( \frac{\left[U\transpose \Sal V \right]_{\kk\el}}{\left[U\transpose \mathbf{1} V \right]_{\kk\el}} \right) 
\]
and $\mathbf{1}$ is the $\g \times \m$ matrix filled with $1$. Confusion matrices $\Rgbz$ and $\Rmbw$ satisfy $\Rgbz \un = \bpi(\bzs)$ and $\Rmbw \un = \brho(\bws)$, with $\un=(1,\ldots, 1)\transpose$ a vector only containing $1$ values, and are obviously in $\mathcal{U}_c$ and $\mathcal{V}_c$ as soon as $(\bzs, \bws)$ is $c/2$ regular. 

The maps $f_{\kk,\el}: (U, V) \mapsto KL(\alskl, \baralkl(U, V))$ are twice differentiable with second derivatives bounded over $\mathcal{U}_c \times\mathcal{V}_c$ and therefore so is $\Lambtilde(U, V)$. Tedious but straightforward computations show that the derivative of $\Lambtilde$ at $(D_{\pii}, D_{\rhoo}) \coloneqq (\Diag(\bpi(\bzs)), \Diag(\brho(\bws)))$ is: 
\begin{align*}
A_{\kk\kp}(\bws) & \coloneqq \frac{\partial \Lambtilde}{\partial U_{\kk\kp}}(D_{\pii}, D_{\rhoo}) = \sum_{\el} \rhol(\bws) \KL\left(\alskl, \als_{\kp\el} \right) \\
B_{\el\lp}(\bzs) & \coloneqq \frac{\partial \Lambtilde}{\partial V_{\el\lp}}(D_{\pii}, D_{\rhoo}) = \sum_{\kk} \rhol(\bzs) \KL\left(\alskl, \als_{\kk\lp} \right)
\end{align*}
$A(\bws)$ and $B(\bzs)$ are the matrix-derivative of $-\Lambtilde/\n\dd$ at $(D_{\pii}, D_{\rhoo})$. Since $(\bzs, \bws)$ is $c/2$-regular and by definition of $\delta(\bals)$, $A(\bws)_{\kk\kp} \geq c\delta(\bals)/2$ (resp. $B(\bws)_{\el\lp} \geq c\delta(\bals)/2$) if $\kk \neq \kp$ (resp. $\el \neq \lp$) and $A(\bws)_{\kk\kk} = 0$ (resp. $B(\bzs)_{\el\el} = 0$) for all $\kk$ (resp. $\el$). By boundedness of the second derivative, there exists $C > 0$ such that for all $(D_{\pii}, D_{\rhoo})$ and all $(H, G) \in B(D_{\pii}, D_{\rhoo}, C)$, we have:
\begin{align*}
\frac{-1}{nd} \frac{\partial \Lambtilde}{\partial U_{\kk\kp}}(H, G) \begin{cases} \geq \frac{3c\delta(\bals)}{8} \text { if } \kk \neq \kp \\ \leq \frac{c\delta(\bals)}{8} \text { if } \kk = \kp \end{cases} \text{ and } \quad 
\frac{-1}{nd} \frac{\partial \Lambtilde}{\partial V_{\el\lp}}(H, G) \begin{cases} \geq \frac{3c\delta(\bals)}{8} \text { if } \el \neq \lp \\ \leq \frac{c\delta(\bals)}{8} \text { if } \el = \lp \end{cases}
\end{align*}
Choose $U$ and $V$ in $(\mathcal{U}_{c} \times \mathcal{V}_{c}) \cap B(D_{\pii}, D_{\rhoo}, C)$ satisfying $U\un = \bpi(\bzs)$ and $V\un = \brho(\bws)$. $U - D_{\pii}$ and $V - D_{\rhoo}$ have nonnegative off diagonal coefficients and negative diagonal coefficients. Furthermore, the coefficients of $U, V, D_{\pii}, D_{\rhoo}$ sum up to $1$ and $\Trace(D_{\pii}) = \Trace(D_{\rhoo}) = 1$. By Taylor expansion, there exists a couple $(H, G)$ also in $(\mathcal{U}_c \times \mathcal{V}_c) \cap B(D_{\pii}, D_{\rhoo}, C)$ such that
\begin{multline*}
\frac{-1}{nd} \Lambtilde\left( U, V \right) =  \frac{-1}{nd} \Lambtilde\left( D_{\pii}, D_{\rhoo} \right) + \Trace\left((U - D_{\pii}) \frac{\partial \Lambtilde}{\partial U}(H, G) \right) + \Trace\left((V - D_{\rhoo}) \frac{\partial \Lambtilde}{\partial V}(H, G) \right) \\
\geq \frac{c\delta(\bals)}{8} [ 3 \sum_{\kk \neq \kp} (U - D_{\pii})_{\kk\kp} + 3 \sum_{\el \neq \lp} (V-D_{\rhoo})_{\el\lp} - \sum_{\kk} (U - D_{\pii})_{\kk\kk} - \sum_{\el} (V-D_{\rhoo})_{\el\el} ] \\
= \frac{c\delta(\bals)}{4} [ (1 - \Trace(U)) + (1 - \Trace(V))]
\end{multline*}
To conclude the proof, assume without loss of generality that $(\bz, \bw) \in S(\bzs, \bws, C)$ achieves the $\|.\|_{0,\sim}$ norm (i.e. it is the closest to $(\bzs, \bws)$ in its representative class). Then $(U, V) = (\Rgbz, \Rmbw)$ is in $(\mathcal{U}_c \times \mathcal{V}_c) \cap B(D_{\pii}, D_{\rhoo}, C)$ and satisfy $U\un = \bpi(\bzs)$ (resp. $V\un = \brho(\bws)$). We just need to note $\n(1 - \Trace(\Rgbz)) = \| \bz - \bzs\|_{0,\sim}$ (resp. $\dd(1 - \Trace(\Rmbw)) = \| \bw - \bws\|_{0,\sim}$) to end the proof. \proofend

%\begin{comment}
The maps $f_{\kk,\el}: x \mapsto KL(\alskl, \normpm(x))$ are twice differentiable with a continuous second derivative bounded by $\vmin^{-2}$ on $\normp(C_\al)$. All terms $\left[U\transpose \Sal V \right]_{\kk\el}\left[U\transpose \mathbf{1} V \right]_{\kk\el}^{-1}$ are convex combinations of the $\normp(\alskl)$ and therefore in $\normp(C_\al)$. Furthermore, their first and second order derivative are also bounded as soon as each row sum of $U$ and $V$ is bounded away from $0$. By composition, all second order partial derivatives of $\Lambtilde$ are therefore continuous and bounded on $\mathcal{U} \times \mathcal{V}$. 

We now compute the first derivative of $\Lambtilde$ at $(D_{\pii}, D_{\rhoo}) \coloneqq (\Diag(\bpi(\bzs)), \Diag(\brho(\bws)))$ by doing a first-order Taylor expansion of $\Lambtilde\left( D_{\pii} + U, D_{\rhoo} + V \right)$ for small $U$ and $V$. 

Tedious but straightforward manipulations show:
\begin{align*}
\baralkl(D_{\pii} + U,  D_{\rhoo} + V) = & \alskl + \frac{1}{\pik(\bzs)} \sum_{\kp} U_{\kk\kp} (S_{\kp\el} - 1) \\
& + \frac{1}{\rhol(\bws)} \sum_{\lp} V_{\el\lp} (S_{\kk\lp} - 1) + \smallO(\|U\|_1, \|V\|_1) \\
\KL\left(\alskl, \baral_{\kp\lp} \right)  = & \KL\left(\alskl, \alskplp \right) + 
  \begin{cases} 
    \bigO(\|U\|_1, \|V\|_1) & \text{if } (\kp, \lp) \neq (\kk, \el) \\
    \smallO(\|U\|_1, \|V\|_1) & \text{if } (\kp, \lp) = (\kk, \el) \\
  \end{cases}
\end{align*}
where the second line comes from the fact that $f_{\kk, \el}'(\normp(\alskl)) = 0$. Keeping only the first order term in $U$ and $V$ in $\Lambtilde$ and noting that $\Lambtilde\left( D_{\pii}, D_{\rhoo}\right) = 0$ yields:

\begin{multline*}
\frac{-1}{nd} [\Lambtilde\left( D_{\pii} + U, D_{\rhoo} + V \right) - \Lambtilde\left( D_{\pii}, D_{\rhoo}\right)] = \frac{-1}{nd} \Lambtilde\left( D_{\pii} + U, D_{\rhoo} + V \right)\\ 
  = \sum_{\kk} D_{\pii,\kk\kk} \sum_{\el, \lp} V_{\el \lp} \KL\left(\alskl, \baral_{\kk\lp} \right) + \sum_{\el} D_{\rhoo,\el\el} \sum_{\kk, \kp} U_{\kk\kp} \KL\left(\alskl, \baral_{\kp\el} \right) + \smallO(\|U\|_1, \|V\|_1) \\
  = \sum_{\kk} \pik(\bzs) \sum_{\el, \lp} V_{\el\lp} \KL\left(\alskl, \als_{\kk\lp}\right) + \sum_{\el} \rhol(\bws) \sum_{\kk, \kp} U_{\kk \kp} \KL\left(\alskl, \als_{\kp\el} \right) + \smallO(\|U\|_1, \|V\|_1) \\
  = \Trace(U A(\bws)) + \Trace(V B(\bzs)) + \smallO(\|U\|_1, \|V\|_1) \\
\end{multline*}
where $A_{\kk\kp}(\bws) \coloneqq  \sum_{\el} \rhol(\bws) \KL\left(\alskl, \als_{\kp\el} \right)$ and $B_{\el\lp}(\bzs) \coloneqq \sum_{\kk} \rhol(\bzs) \KL\left(\alskl, \als_{\kk\lp} \right)$. $A$ and $B$ are the matrix-derivative of $-\Lambtilde/\n\dd$ at $(D_{\pii}, D_{\rhoo})$. Since $(\bzs, \bws)$ is $c/2$-regular and by definition of $\delta(\bals)$, $A_{\kk\kp} \geq c\delta(\bals)/2$ for $\kk \neq \kp$ and $B_{\el\lp} \geq c\delta(\bals)/2$ for $\el \neq \lp$ and the diagonal terms of $A$ and $B$ are null. By boundedness of the lower second derivative of $\Lambtilde$, there exists a constant $C > 0$ such that for all $(H, G) \in B(D_{\pii}, D_{\rhoo}, C)$, we have:
\begin{equation*}
\frac{-1}{nd} \frac{\partial \Lambtilde}{\partial U_{\kk\kp}}(H, G) \begin{cases} \geq \frac{3c\delta(\bals)}{8} \text { if } \kk \neq \kp \\ \leq \frac{c\delta(\bals)}{8} \text { if } \kk = \kp \end{cases} \text{ and } \quad 
\frac{-1}{nd} \frac{\partial \Lambtilde}{\partial V_{\el\lp}}(H, G) \begin{cases} \geq \frac{3c\delta(\bals)}{8} \text { if } \el \neq \lp \\ \leq \frac{c\delta(\bals)}{8} \text { if } \el = \lp \end{cases}
\end{equation*}
In particular, if $U$ and $V$ have nonnegative non diagonal coefficients and negative diagonal coefficients. 
\begin{multline*}
\frac{-1}{nd}\left[ \Trace\left(U \frac{\partial \Lambtilde}{\partial U}(H, G) \right) + \Trace\left(V \frac{\partial \Lambtilde}{\partial V}(H, G) \right) \right]\\\geq \frac{c\delta(\bals)}{4} \left[ \sum_{\kk\kp} U_{\kk\kp} + \sum_{\el\lp} V_{\el\lp} - \Trace(U) - \Trace(V)\right]
\end{multline*}
Choose $U$ and $V$ in $(\mathcal{U} \times \mathcal{V}) \cap B(D_{\pii}, D_{\rhoo}, c_3)$ satisfying $U\un = \bpi(\bzs)$ and $V\un = \brho(\bws)$. Note that $U - D_{\pii}$ and $V - D_{\rhoo}$ have nonnegative non diagonal coefficients, negative diagonal coefficients, that their coefficients sum up to $1$ and that $\Trace(D_{\pii}) = \Trace(D_{\rhoo}) = 1$. By Taylor expansion, there exists a couple $(H, G)$ also in $(\mathcal{U} \times \mathcal{V}) \cap B(D_{\pii}, D_{\rhoo}, C)$ such that
\begin{multline*}
\frac{-1}{nd} \Lambtilde\left( U, V \right) =  \frac{-1}{nd} \Lambtilde\left( D_{\pii} + (U - D_{\pii}), D_{\rhoo} + (V - D_{\rhoo}) \right) \\
= \Trace\left((U - D_{\pii}) \frac{\partial \Lambtilde}{\partial U}(H, G) \right) + \Trace\left((V - D_{\rhoo}) \frac{\partial \Lambtilde}{\partial V}(H, G) \right) \\
\geq \frac{c\delta(\bals)}{4} [ \sum_{\kk,\kp} (U - D_{\pii})_{\kk\kp} + \sum_{\el,\lp} (V-D_{\rhoo})_{\el\lp} - \Trace(U - D_{\pii}) - \Trace(V - D_{\rhoo}) ] \\
= \frac{c\delta(\bals)}{4} [ (1 - \Trace(U)) + (1 - \Trace(V))]
\end{multline*}
To conclude the proof, choose any assignment $(\bz, \bw)$ and without loss of generality assume that $(\bz, \bw)$ are closest to $(\bzs,\bws)$ in their equivalence class. Then $\Rgbz$ is in $\mathcal{U}$ and additionally satifies $\Rgbz \un = \bpi(\bzs)$ and $\| \bz - \bzs\|_{0,\sim} = \n \|\Rgbz - D_{\pii}\|_1 /2 = \n(1 - \Trace(\Rgbz))$. Similar equalities hold for $\Rmbw$ and $\| \bw - \bws\|_{0}$. \proofend

%\end{comment}

\subsection{Proof of Proposition~\ref{prop:conditional-likelihood-convergence} (global convergence $\Fnd$)}

\proofbegin
Conditionally upon $(\bzs, \bws)$,
\begin{align*}
   \Fnd(\btheta, \bz, \bw) - \Lambtilde(\bz,\bw) & \leq \Fnd(\btheta, \bz, \bw) - \G(\btheta, \bz,\bw) \\
  & = \sum_{\ii} \sum_{\jj} (\al_{\zi\wj} - \al^\vrai_{\zi^\vrai \wj^\vrai}) \left(x_{ij} - \normp(\al^\vrai_{\zi^\vrai \wj^\vrai}) \right)  \\
  & = \sum_{\kk \kp} \sum_{\el \lp} \left(\al_{\kp\lp} - \al^\vrai_{\kk \el} \right) W_{\kk \kp \el \lp} \\
  & \leq \sup_{\substack{\Gamma \in \R^{\g^2 \times \m^2} \\ \| \Gamma \|_{\infty} \leq \Diam(\bTheta)}} \sum_{\kk \kp} \sum_{\el \lp} \Gamma_{\kk \kp \el \lp} W_{\kk \kp \el \lp} \coloneqq Z
\end{align*}
uniformly in $\btheta$, where the $W_{\kk \kp \el \lp}$ are independent and defined by:
\begin{equation*}
  W_{\kk \kp \el \lp} = \sum_{\ii} \sum_{\jj} \z^\vrai_{\ii \kk} \w^\vrai_{\jj \el} \z_{\ii, \kp} \w_{\jj \lp}\left(x_{ij} - \normp(\al^\vrai_{\kk\el}) \right)
\end{equation*}
is the sum of $\n\dd \Rgbz_{\kk\kp} \Rmbw_{\el\lp}$ sub-exponential variables with parameters $(\vmax^2, 1/\neighborsize)$ and is therefore itself sub-exponential with parameters $(\n\dd \Rgbz_{\kk\kp} \Rmbw_{\el\lp} \vmax^2, 1 / \neighborsize)$. According to Proposition~\ref{prop:concentration-subexponential}, $\Esp_{\bthetas}[Z|\bzs, \bws] \leq \g \m \Diam(\bTheta) \sqrt{nd \vmax^2}$ and 
$Z$ is sub-exponential with parameters $(\n\dd \Diam(\bTheta)^2 (2\sqrt{2})^2\vmax^2, 2\sqrt{2}\Diam(\bTheta) / \neighborsize)$. In particular, for all $\vareps_{\n,\dd} < \neighborsize / 2\sqrt{2}\Diam(\bTheta)$
\begin{multline*}
  \Prob_{\bthetas}\left( \left. Z \geq \vmax \g \m \Diam(\bTheta) \sqrt{nd} \left\{ 1 + \frac{\sqrt{8\n\dd} \vareps_{\n,\dd}}{\g \m} \right\} \right| \bzs, \bws \right) \\
  \leq  \Prob_{\bthetas}\left( \left. Z \geq \Esp_{\bthetas}[Z|\bzs, \bws] + \vmax \Diam(\bTheta) \n\dd 2\sqrt{2}\vareps_{\n,\dd} \right| \bzs, \bws \right) \\  \leq \exp \left( - \frac{\n\dd\vareps^2_{\n,\dd}}{2} \right)
\end{multline*}
We can then remove the conditioning and take a union bound to prove Equation~\eqref{eq:conditional-likelihood-convergence}. 
\proofend

\subsection{Proof of Proposition~\ref{prop:large-deviations-profile-likelihood} (contribution of far away assignments)}

\proofbegin
Conditionally on $(\bzs, \bws)$, we know from
proposition~\ref{prop:maximum-conditional-likelihood} that
$\Lambtilde$ is maximal in $(\bzs, \bws)$ and its equivalence
class. Choose $0 < \tnd$ decreasing to $0$ but satisfying $\tnd \gg \max\left(\frac{\n+\dd}{\n\dd}, \frac{\log(\n\dd)}{\sqrt{\n\dd}}\right)$. This is always possible because we assume that $\log(\dd)/\n\rightarrow 0$ and $\log(\n)/\dd\rightarrow 0$. According to \ref{prop:maximum-conditional-likelihood} (iii), for all $(\bz, \bw) \notin (\bzs, \bws, \tnd)$
\begin{equation}
\label{eqn:maxlambdatilde}
\Lambtilde(\bz, \bw) \leq - \frac{c \delta(\bals)}{4}(\n \|\bw - \bws\|_{0, \sim} + \dd \|\bz - \bzs\|_{0, \sim}) \leq - \frac{c \delta(\bals)}{4} \n\dd\tnd
\end{equation}
since either $\|\bz - \bzs\|_{0, \sim} \geq \n \tnd$ or $\|\bw - \bws\|_{0, \sim} \geq \dd \tnd$. 

% 
% By continuity \CK{?? separation cf proposition \ref{prop:maximum-conditional-likelihood} (iii) . et: est-on sur qu'on peut avoir des $\Lambtilde$ de cet ordre de grandeur?} of
% $\Lambtilde$ and since $\Lambtilde(\bzs, \bws) = 0$, there exist a $\tilde{c} > 0$ such that
% \begin{equation*}
% \forall (\bz, \bw) \notin S(\bzs, \bws, \tilde{c}) \quad \Lambtilde(\bz, \bw) \leq - 2 \n \dd \tnd.
% \end{equation*}
% If additionally $\tilde{c} \leq c_3$, using the definition of $S(\bzs, \bws, \tilde{c})$ , we get 
% \begin{equation}
% \label{eqn:maxlambdatilde}
% \sup_{S^{c}(\bzs, \bws, \tilde{c})} \Lambtilde(\bz, \bw) \leq - \frac{c \tilde{c} \delta(\bals)}{4}\n\dd \ll - \n \dd \tnd
% \end{equation}
% thanks to proposition~\ref{prop:profile-likelihood-derivative}. 
% 
% 
% \CK{Attention: si on n'est pas sur $S$, c'est que $\inf_{\bz' \sim \bz} \|\bz' - \bzs\|_0 > \tilde{c} \n$  OU (et on ET) $\inf_{\bw' \sim \bw} \|\bw' - \bws\|_0 \leq \tilde{c} \dd$. et donc, je ne vois pas comment on arrive au facteur en $nd$. }We can therefore choose $\tilde{c} \leq \min(c_3, c/4)$ without loss of generality. 

Set $\vareps_{n\dd} = \frac{\inf( c\delta(\bals) \tnd / 16 \vmax, \neighborsize)}{\Diam(\bTheta)}$. By proposition~\ref{prop:conditional-likelihood-convergence}, and with our choice of $\vareps_{n\dd}$, 
with probability higher than $1 - \Delta_{\n\dd}^1(\vareps_{\n\dd})$,
\begin{align*}
  &\sum_{(\bz, \bw) \notin S(\bzs, \bws, \tnd)} \prob(\bx, \bz, \bw; \btheta)\\ 
  & = \prob(\bx| \bzs, \bws, \bthetas) \sum_{(\bz, \bw) \notin S(\bzs, \bws, \tnd)} \prob(\bz, \bw; \btheta) e^{\Fnd(\btheta, \bz, \bw) - \Lambtilde(\bz, \bw) + \Lambtilde(\bz, \bw)} \\
  & \leq \prob(\bx| \bzs, \bws, \bthetas) \sum_{\bz, \bw} \prob(\bz, \bw; \btheta) e^{\Fnd(\btheta, \bz, \bw) - \Lambtilde(\bz, \bw) - \n \dd \tnd c\delta(\bals) / 4} \\
  & \leq \prob(\bx| \bzs, \bws, \bthetas) \sum_{\bz, \bw} \prob(\bz, \bw; \btheta) e^{\n \dd \tnd c\delta(\bals) / 8} \\
  & = \frac{\prob(\bx, \bzs, \bws; \bthetas)}{\prob(\bzs, \bws; \bthetas)} e^{- \n \dd \tnd c\delta(\bals) / 8} \\
  & \leq \prob(\bx, \bzs, \bws; \bthetas) \exp\left( -\n\dd \tnd \frac{c\delta(\bals)}{8} + (\n+\dd)\log \frac{1-c}{c} \right) \\
  & = \prob(\bx, \bzs, \bws; \bthetas) \smallO(1)
\end{align*}
where the second line comes from inequality (\ref{eqn:maxlambdatilde}), the third from the global control studied in Proposition~\ref{prop:conditional-likelihood-convergence} and the definition of $\vareps_{\n\dd}$, the fourth from the definition of $\prob(\bx, \bzs, \bws; \bthetas)$, the fifth from the bounds on $\bpis$ and $\brhos$ and the last from $\tnd \gg (\n+\dd)/\n\dd$. 

In addition, we have $\vareps_{n\dd} \gg \log(\n\dd) / \sqrt{\n\dd}$ so that the series $\sum_{\n, \dd} \Delta_{\n\dd}^1(\vareps_{\n\dd})$ converges and:
\begin{align*}
  \sum_{(\bz, \bw) \notin S(\bzs, \bws, \tnd)} \prob(\bx, \bz, \bw; \btheta) & = \prob(\bx; \bzs, \bws, \bthetas) \smallO_P(1)
\end{align*}
\proofend
  
\subsection{Proof of Proposition~\ref{prop:profile-likelihood-convergence-local} (local convergence $\Fnd$)} 

\proofbegin
We work conditionally on $(\bzs, \bws) \in \mcZ_1 \times \mcW_1$. Choose $\vareps \leq \neighborsize \vmin^2$ small. Assignments $(\bz, \bw)$ at $\|.\|_{0,\sim}$-distance less than $c/4$ of $(\bzs, \bws)$ are $c/4$-regular. According to Proposition~\ref{proposition:maxzw}, $\hxkl$ and $\barxkl$ are at distance at most $\vareps$ with probability higher than $1 - \exp \left( - \frac{\n\dd c^2 \vareps^2}{128(\vmax^2 + \neighborsize^{-1} \vareps)}\right)$. Manipulation of $\Lamb$ and $\Lambtilde$ yield
\begin{align*}
  \frac{\Fnd(\btheta, \bz, \bw) - \Lambtilde(\bz, \bw)}{\n\dd} & \leq \frac{\Lamb(\bz, \bw) - \Lambtilde(\bz, \bw)}{\n\dd} \\
  & = \sum_{\kk, \kp'} \sum_{\el, \lp} \Rgbz_{\kk\kp} \Rmbw_{\el\lp} \left[ f_{\kk\el}(\hx_{\kp\lp}) - f_{\kk\el}(\barx_{\kp\lp}) \right]
\end{align*}
where $f_{\kk\el}(x) = - \Salkl \normpm(x) + \norm\circ\normpm(x)$. The functions $f_{\kk\el}$ are twice differentiable on $\mathring{\mathcal{A}}$ with bounded first and second derivatives over $I = \normp([-M_{\al}, M_{\al}])$ so that:
\begin{equation*}
  f_{\kk\el}(y) - f_{\kk\el}(x) = f'_{\kk\el}(x) \left( y - x \right) + \smallO\left( y - x \right)
\end{equation*}
where the $\smallO$ is uniform over pairs $(x,y) \in I^2$ at distance less than $\vareps$ and does not depend on $(\bzs, \bws)$. $\barxkl$ is a convex combination of the $\Salkl = \normp(\alskl) \in \normp(C_{\al})$. Since $\normp$ is monotonic, $\barxkl \in \normp(C_\al) \subset I$. Similarly, $| \hxkl - \barxkl | \leq \neighborsize \vmin^2$ and $|\normp'| \geq \vmin^2$ over $I$ therefore $\hxkl \in I$. We now bound $f'_{\kk\el}$:
\begin{align*}
  | f'_{\kk\el}(\barx_{\kp\lp}) | &= \left| \frac{\barx_{\kp\lp} - \Salkl}{\normp' \circ \normpm (\barx_{\kp\lp})} \right|  = \left| \frac{\frac{\left[ \Rgbz\transpose \Sal \Rmbw \right]_{\kp\lp}}{\pizk\rhowl} - \Salkl}{\normp' \circ \normpm (\barx_{\kp\lp})} \right| \\
  & \leq \left( 1 - \frac{\Rgbz_{\kk\kp}\Rmbw_{\el\lp}}{\pizk\rhowl}\right) \frac{\Salmax - \Salmin}{\vmin^2}
\end{align*}
where $\Salmax = \max_{\kk,\el} \Salkl$ and $\Salmin = \min_{\kk,\el} \Salkl$. In particular,
\begin{align*}
  \Rgbz_{\kk\kp}\Rmbw_{\el\lp} | f'_{\kk\el}(\barx_{\kp\lp}) | & \leq \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \left( 1 - \frac{\Rgbz_{\kk\kp}\Rmbw_{\el\lp}}{\pizk\rhowl}\right) \frac{\Salmax - \Salmin}{\vmin^2}  \\
  & \leq
  \begin{cases}
    \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \frac{\Salmax - \Salmin}{\vmin^2} & \text{if } (\kp, \lp) \neq (\kk, \el) \\
    \left[ \pizk \rhowl - \Rgbz_{\kk\kk}\Rmbw_{\el\el} \right]  \frac{\Salmax - \Salmin}{\vmin^2} & \text{if } (\kk, \el) = (\kk, \el)
  \end{cases}
\end{align*}
Wrapping everything,
\begin{align*}
\frac{| \Lamb(\bz, \bw) - \Lambtilde(\bz, \bw) | }{\n\dd} & = \left| \sum_{\kk,\kp} \sum_{\el,\lp} \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \left[ f'_{\kk\el}(\barx_{\kp\lp}) (\hxkl -\barxkl) + \smallO(\hxkl -\barxkl) \right]\right| \\
& \leq \left[ \sum_{(\kp,\lp) \neq (\kk, \el)} \Rgbz_{\kk\kp}\Rmbw_{\el\lp} + \sum_{\kk, \el} ( \pizk \rhowl - \Rgbz_{\kk\kk}\Rmbw_{\el\el} ) \right]\\ & \times \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
& = 2 \left[ \sum_{(\kp,\lp) \neq (\kk, \el)} \Rgbz_{\kk\kp}\Rmbw_{\el\lp} \right] \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
& = 2 \left[ 1 - \Trace(\Rgbz)\Trace(\Rmbw) \right] \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
& \leq 2\left( \frac{\|\bz -\bzs\|}{\n} + \frac{\|\bw -\bws\|}{\dd} \right) \frac{\Salmax - \Salmin}{\vmin^2} \max_{\kk, \el} | \hxkl -\barxkl | (1 +\smallO(1)) \\
& \leq 2\left( \frac{\|\bz -\bzs\|}{\n} + \frac{\|\bw -\bws\|}{\dd} \right) \frac{\Salmax - \Salmin}{\vmin^2} \vareps (1 +\smallO(1))
\end{align*}
We can remove the conditioning on $(\bzs, \bws)$ to prove Equation~\eqref{eq:profile-likelihood-convergence-local} with $c_2 = 2(\Salmax - \Salmin) / \vmin^2$ and $c_1 = c_2^2$.
\proofend

\subsection{Proof of Proposition~\ref{prop:small-deviations-profile-likelihood} (contribution of local assignments)} 

\proofbegin
By Proposition~\ref{cor:prob-regular-configurations-star}, it is enough to prove that the sum is small compared to $\prob(\bzs, \bws, \bx; \bthetas)$ on $\Om_1$. We work conditionally on $(\bzs, \bws) \in \mcZ_1 \times \mcW_1$. Choose $(\bz, \bw)$ in $S(\bzs, \bws, C)$ with $C$ defined in proposition~\ref{prop:large-deviations-profile-likelihood}. 
\begin{align*}
  \log \left( \frac{\prob(\bz, \bw, \bx; \btheta)}{\prob(\bzs, \bws, \bx; \bthetas)} \right) & = \log \left( \frac{\prob(\bz, \bw; \btheta)}{\prob(\bzs, \bws; \bthetas)}\right) + \Fnd(\btheta, \bz, \bw) \\
\end{align*}
For $C$ small enough, we can assume without loss of generality that $(\bz, \bw)$ is the representative closest to $(\bzs, \bws)$ and note $r_1 = \|\bz - \bzs\|_0$ and $r_2 = \|\bw - \bws\|_0$. We choose $\vareps_{\n\dd} \leq \min(\neighborsize \vmin^2, c\delta(\bals)/8)$. Then with probability at least $1 - \exp\left( -\frac{\n\dd\tilde{c}^2\vareps_{\n\dd}^2}{8(c_1\vmax^2 + c_2 \neighborsize^{-1} \vareps_{\n\dd})} \right)$:
\begin{align*}
  \Fnd(\btheta, \bz, \bw) & \leq \Lamb(\bz, \bw) - \Lambtilde(\bz, \bw) + \Lambtilde(\bz, \bw) \\
  & \leq \Lamb(\bz, \bw) - \Lambtilde(\bz, \bw) - \frac{c\delta(\bals)}{4}\left(\dd r_1 + \n r_2\right) \\
  & \leq \vareps_{\n\dd} \left(\dd r_1 + \n r_2\right) - \frac{c\delta(\bals)}{4}\left(\dd r_1 + \n r_2\right) \\
  & \leq - \frac{c\delta(\bals)}{8}\left(\dd r_1 + \n r_2\right) \\
\end{align*}
where the first line comes from the definition of $\Lamb$, the second line from Proposition~\ref{prop:maximum-conditional-likelihood}, the third from Proposition~\ref{prop:profile-likelihood-convergence-local} and the last from $\vareps_{\n\dd} \leq c\delta(\bals)/8$. A union bound shows that
\begin{multline*}
  \Delta_{\n\dd}(\vareps_{\n\dd}) = \Prob_{\bthetas} \left( \sup_{\substack{(\bz, \bw) \in (\bzs, \bws, \tilde{c}) \\ \btheta \in \bTheta}} \Fnd(\btheta, \bz, \bw) \geq - \frac{c\delta(\bals)}{8}\left(\dd \|\bz - \bzs\|_{0,\sim}  + \n \|\bw - \bws\|_{0, \sim} \right) \right) \\ \leq \g^{\n}\m^{\dd} \exp\left( - \frac{\n\dd\tilde{c}^2\vareps_{\n\dd}^2}{8(c_1\vmax^2 + c_2 \neighborsize^{-1} \vareps_{\n\dd})} \right)
\end{multline*}

Thanks to corollary~\ref{cor:marginalprobabilties}, we also know that:
\begin{equation*}
 \log \left( \frac{\prob(\bz, \bw; \btheta)}{\prob(\bzs, \bws; \bthetas)}\right) \leq \bigO_P(1) \exp\left\{M_{c/4}(r_1 + r_2) \right\}
\end{equation*}
There are at most ${\n \choose r_1}{\n \choose r_2}\g^{r_1}\m^{r_2}$ assignments $(\bz,\bw)$ at distance $r_1$ and $r_2$ of $(\bzs, \bws)$ and each of them has at most $\g^{\g} \m^{\m}$ equivalent configurations. Therefore, with probability $1 - \Delta_{\n\dd}(\vareps_{\n\dd})$,
\begin{align*}
 & \frac{\sum_{\substack{(\bz, \bw) \in S(\bzs, \bws, \tilde{c}) \\ (\bz, \bw) \nsim (\bzs, \bws)}} \prob(\bz, \bw, \bx; \btheta)}{\prob(\bzs, \bws, \bx; \bthetas)} \\
  & \leq \bigO_P(1) \sum_{\substack{r_1 + r_2 \geq 1}} {\n \choose r_1}{\n \choose r_2}\g^{\g + r_1}\m^{\m + r_2} \exp\left( (r_1 + r_2)M_{c/4} - \frac{c\delta(\bals)}{8}\left(\dd r_1 + \n r_2\right) \right) \\
  & = \bigO_P(1) \left(1+ e^{(\g+ 1)\log \g + M_{c/4} - \dd \frac{c \delta(\bals)}{8} }\right)^{\n}
  \left(1+ e^{(\m + 1)\log \m + M_{c/4} - \n \frac{c \delta(\bals)}{8} }\right)^{\dd} -1 \\
  & \leq \bigO_P(1) a_{\n\dd} \exp(a_{\n\dd})
\end{align*}
where $a_{\n\dd} = \n e^{(\g + 1)\log \g + M_{c/4} - \dd \frac{c \delta(\bals)}{8} } + \dd e^{(\m + 1)\log \m + M_{c/4} - \n \frac{c \delta(\bals)}{8} } = \smallO(1)$ as soon as $\n \gg \log \dd$ and $\dd \gg \log \n$. If we take $\vareps_{\n\dd} \gg \log(\n\dd)/\sqrt{\n\dd}$, the series $\sum_{\n,\dd} \Delta_{\n\dd}(\vareps_{\n\dd})$ converges which proves the results.
\proofend

\subsection{Proof of Proposition~\ref{prop:equivalent-configurations-profile-likelihood} (contribution of equivalent assignments)} 

\proofbegin
Choose $(s, t)$ permutations of $\{1, \dots, \g\}$ and $\{1, \dots, \m\}$ and assume that $\bz = \bz^{\vrai,s}$ and $\bw = \bw^{\vrai, t}$. Then $\prob(\bx, \bz, \bw; \btheta) = \prob(\bx, \bz^{\vrai, s}, \bw^{\vrai, t}; \btheta) = \prob(\bx, \bzs, \bws; \btheta^{s,t})$.  If furthermore $(s, t) \in \Symmetric(\btheta)$, $\btheta^{s, t} = \btheta$ and immediately $\prob(\bx, \bz, \bw; \btheta) = \prob(\bx, \bzs, \bws; \btheta)$. We can therefore partition the sum as 

\begin{align*}
  \sum_{(\bz, \bw) \sim (\bz, \bw)} \prob(\bx, \bz, \bw; \btheta) & = \sum_{s, t} \prob(\bx, \bz^{\vrai, s}, \bw^{\vrai, t}; \btheta) \\ 
  & = \sum_{s, t} \prob(\bx, \bzs, \bws; \btheta^{s,t}) \\ 
  & = \sum_{\btheta' \sim \btheta} \# \Symmetric(\btheta') \prob(\bx, \bzs, \bws; \btheta') \\ 
  & = \# \Symmetric(\btheta) \sum_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') \\ 
\end{align*}

$\prob(\bx, \bzs, \bws; \btheta)$ unimodal in $\btheta$, with a mode in $\bthetaMC$. By consistency of $\bthetaMC$, either $\prob(\bx, \bzs, \bws; \btheta) = \smallO_P(\prob(\bx, \bzs, \bws; \bthetas))$ or $\prob(\bx, \bzs, \bws; \btheta) = \bigO_P(\prob(\bx, \bzs, \bws; \bthetas))$ and $\btheta \to \bthetas$. In the latter case, any $\btheta' \sim \btheta$ other than $\btheta$ is bounded away from $\bthetas$ and thus $\prob(\bx, \bzs, \bws; \btheta') = \smallO_P(\prob(\bx, \bzs, \bws; \bthetas))$. In summary, 
\[
 \sum_{\btheta' \sim \btheta} \frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \bthetas)} = \max_{\btheta' \sim \btheta} \frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \bthetas)} (1 + \smallO_P(1))
\] \proofend

\subsection{Proof of Corollary~\ref{cor:behaviorEMV}: Behavior of $\bthetaEMV$}
\label{annexe:cor:behaviorEMV}

%We may prove the lemma by establishing a contradiction: if for every permutations $(s, t)$ of $\{1, \dots, \g\}$ and $\{1, \dots, \m\}$, we suppose that $\hat{\bpi}\left(\bzs\right)-\bpiEMV^{s}\neq o\left(\n^{-1/2}\right)$ or $\hat{\brho}\left(\bws\right)-\brhoEMV^{t}\neq o\left(\dd^{-1/2}\right)$ or $\hat{\bal}\left(\bzs,\bws\right)-\balEMV^{s,t}\neq o\left(\left(\n\dd\right)^{-1/2}\right)$, then, by the Poposition~\ref{prop:LocalAsymp} and the consistency of $\hat{\btheta}\left(\bzs,\bws\right)$
%\begin{equation}\label{eq:proof:ConsistenceEMV}
%\Lcs\left(\hat{\btheta}\left(\bzs,\bws\right)\right)- \Lcs\left(\bthetaEMV^{s,t}\right)=\Omega_{P}(1).
%\end{equation}
%
%But, since $\hat{\btheta}\left(\bzs,\bws\right)$ and $\bthetaEMV$, respectively, maximise $\frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \bthetas)}$ and $\frac{\prob(\bx; \btheta)}{\prob\left(\bx; \btheta^{\vrai}\right)}$, it follows by corollary~\ref{cor:observed-akin-to-complete-simple-case} that for some permutation $(s,t)$, we have
%\[\left|\frac{\prob\left(\bx, \bzs, \bws; \hat{\btheta}\left(\bzs,\bws\right)\right)}{\prob(\bx, \bzs, \bws; \bthetas)}-\frac{\prob\left(\bx, \bzs, \bws; \bthetaEMV^{s,t}\right)}{\prob(\bx, \bzs, \bws; \bthetas)}\right|=\smallO_P(1)\]
%that implicates a contradiction with Eq~(\ref{eq:proof:ConsistenceEMV}) and concludes the proof.

Theorem~\ref{thm:observed-akin-to-complete-general}, states that:
\begin{equation*}
    \frac{\prob(\bx; \btheta)}{\prob(\bx; \bthetas)} = \frac{\# \Symmetric(\btheta)}{\# \Symmetric(\bthetas)} \max_{\btheta' \sim \btheta} \frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \bthetas)}\left(1 + \smallO_P(1)\right) + \smallO_P(1)
\end{equation*}
Then,
\begin{eqnarray*}
\prob(\bx; \btheta)&=&\# \Symmetric(\btheta)\frac{\prob(\bx; \bthetas)}{\# \Symmetric(\bthetas)\prob(\bx, \bzs, \bws; \bthetas)} \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta')\left(1 + \smallO_P(1)\right) + \smallO_P(1)\\
&=&\# \Symmetric(\btheta)\frac{1}{\# \Symmetric(\bthetas)\prob( \bzs, \bws|\bx; \bthetas)} \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta')\left(1 + \smallO_P(1)\right) + \smallO_P(1).\\
\end{eqnarray*}
Now, using Corollary 3 p. 553 of Mariadassou and Matias~\cite{mariadassou2015}  
\[p(\cdot,\cdot|\bx;\bthetas)\overset{({\cal D})}{\underset{\n,\dd\to+\infty}{\longrightarrow}}\frac{1}{\#\Symmetric(\bthetas)}\sum_{(\bz,\bw)\overset{\bthetas}{\sim}(\bzs,\bws)}{\delta_{(\bz,\bw)}(\cdot,\cdot)},\]
 we can deduce that
\begin{eqnarray}
\nonumber
\prob(\bx; \btheta)
&=&\# \Symmetric(\btheta)\frac{1}{\# \Symmetric(\bthetas)\prob( \bzs, \bws|\bx; \bthetas)} \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta')\left(1 + \smallO_P(1)\right) + \smallO_P(1)\\
\nonumber
&=&\# \Symmetric(\btheta)\frac{1}{1 + \smallO_P(1)} \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta')\left(1 + \smallO_P(1)\right) + \smallO_P(1)\\
\label{Eq:appendix:emv}
&=&\# \Symmetric(\btheta) \max_{\btheta' \sim \btheta}\prob(\bx, \bzs, \bws; \btheta')\left(1 + \smallO_P(1)\right) + \smallO_P(1).
\end{eqnarray}
Finaly, we conclude with the proposition~\ref{prop:LocalAsymp}.



\subsection{Proof of Corollary~\ref{cor:Variational}: Behavior of $\Jvar\left(\setQ,\btheta\right)$}\label{annexe:cor:Variational}

Remark first that for every $\btheta$ and for every $\left(\bz,\bw\right)$, 
\[\prob\left(\bx,\bz,\bw;\btheta\right)\leq\exp\left[\Jvar\left(\delta_{\bz}\times\delta_{\bw},\btheta\right)\right]
\leq \underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]\leq\prob\left(\bx;\btheta\right)\]
where $\delta_{\bz}$ denotes the dirac mass on $\bz$. By dividing by $\prob\left(\bx;\bthetas\right)$, we obtain
\[\frac{\prob\left(\bx,\bz,\bw;\btheta\right)}{\prob\left(\bx;\bthetas\right)}
\leq \frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]}{\prob\left(\bx;\bthetas\right)} \leq\frac{\prob\left(\bx;\btheta\right)}{\prob\left(\bx;\bthetas\right)}.\]
As this inequality is true for every couple $(\bz,\bw)$, we have:
\[\underset{(\bz,\bw)\in\mcZ\times\mcW}{\max}\frac{\prob\left(\bx,\bz,\bw;\btheta\right)}{\prob\left(\bx;\bthetas\right)}
\leq \frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]}{\prob\left(\bx;\bthetas\right)} .\]
Moreover, using Equation \ref{Eq:appendix:emv}, we get a lower bound:
\begin{eqnarray*}
\underset{(\bz,\bw)\in\mcZ\times\mcW}{\max}\frac{\prob\left(\bx,\bz,\bw;\btheta\right)}{\prob\left(\bx;\bthetas\right)}&=&\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\prob\left(\bx;\bthetas\right)}+o_p(1)\\
&=&\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\#\Symmetric(\bthetas)\prob\left(\bx,\bzs,\bws;\bthetas\right)\left(1+o_p(1)\right)}+o_p(1)\\
&=&\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\#\Symmetric(\bthetas)\prob\left(\bx,\bzs,\bws;\bthetas\right)}+o_p(1).\\
\end{eqnarray*}
Now, Theorem~\ref{thm:observed-akin-to-complete-general} leads to the following upper bound:
\begin{eqnarray*}
\frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]}{\prob\left(\bx;\bthetas\right)} &\leq&\frac{\prob\left(\bx;\btheta\right)}{\prob\left(\bx;\bthetas\right)}\\
&\leq&\frac{\# \Symmetric(\btheta)}{\# \Symmetric(\bthetas)}\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}+o_p(1)\\
\end{eqnarray*}
so that we have the following control
\begin{eqnarray*}
\!\!\!\!\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\# \Symmetric(\bthetas)\prob\left(\bx,\bzs,\bws;\bthetas\right)}+o_p(1)\leq &\!\!\!\!&\!\!\!\!\!\!\!\!\!\!\!\!\frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]}{\prob\left(\bx;\bthetas\right)}\\
&\!\!\!\!&\!\!\!\! \leq\frac{\# \Symmetric(\btheta)}{\# \Symmetric(\bthetas)}\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}+o_p(1).\\
\end{eqnarray*}
In the particular case where $\# \Symmetric(\btheta)=1$, we have
\[\frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]}{\prob\left(\bx;\bthetas\right)} =\frac{1}{\# \Symmetric(\bthetas)}\underset{\btheta'\sim\btheta}{\max}\frac{\prob\left(\bx,\bzs,\bws;\btheta'\right)\left(1+o_p(1)\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}+o_p(1)\]
and, following the same reasoning as the appendix~\ref{annexe:cor:behaviorEMV}, we have the result.\\

\begin{comment}
To prove the Equation \ref{cor:Variational:plus}, first write
\[\Jvar\left(\setQ,\btheta\right)=\log\prob(\bx;\btheta)-KL\left(\setQ,\prob(\cdot,\cdot|\bx,\btheta)\right)\]
that implies
\begin{eqnarray*}
\underset{\setQ\in\mcQ}{\max}\;\Jvar\left(\setQ,\btheta\right)-\log\prob(\bx;\btheta)&=&
\underset{\setQ\in\mcQ}{\max}-KL\left(\setQ,\prob(\cdot,\cdot|\bx,\btheta)\right)\\
&=&
-\underset{\setQ\in\mcQ}{\min}KL\left(\setQ,\prob(\cdot,\cdot|\bx,\btheta)\right)\\
&\geq&-KL\left(\delta_{\bzs,\bws},\prob(\cdot,\cdot|\bx,\btheta)\right)\\
&\geq&-\sum_{(\bz,\bw)\in\mcZ\times\mcW}{\delta_{(\bzs,\bws)}(\bz,\bw)\log\left(\frac{\delta_{(\bzs,\bws)}(\bz,\bw)}{\prob\left(\bz,\bw|\bx;\btheta\right)}\right)}\\
&\geq&\log\left(\prob\left(\bzs,\bws|\bx;\btheta\right)\right).\\
\end{eqnarray*}
If $\btheta\approx\bthetas$ in the sense of \cite{mariadassou2015}, that is to say $\bal\approx\bals$, we have
\[p(\cdot,\cdot|\bx;\bthetas)\overset{({\cal D})}{\underset{\n,\dd\to+\infty}{\longrightarrow}}\frac{1}{\#\Symmetric(\bthetas)}\sum_{(\bz,\bw)\overset{\bthetas}{\sim}(\bzs,\bws)}{\delta_{(\bz,\bw)}(\cdot,\cdot)}.\]
Thus, for $\btheta\approx\bthetas$,
\begin{eqnarray*}
\limsup_{\n,\dd\to\infty}\underset{\setQ\in\mcQ}{\max}\;\Jvar\left(\setQ,\btheta\right)-\log\prob(\bx;\btheta)&\geq&-\log\left(\#\Symmetric(\bthetas)\right)\geq -\log\left(\g!\m!\right).\\
\end{eqnarray*}

When $\btheta\not{\!\!\approx}\bthetas$, 
\begin{eqnarray*}
\Jvar\left(\setQ_1,\btheta\right)-\underset{\setQ_2\in\mcQ}{\max}\;\Jvar\left(\setQ_2,\bthetas\right)&=&
\log \prob(x;\btheta)-\KL\left(\setQ_1,p(\cdot,\cdot|\bx,\btheta)\right)-\left[\log \prob(x;\bthetas)-\underset{\setQ_2\in\mcQ}{\min}\;\KL\left(\setQ_2,p(\cdot,\cdot|\bx,\bthetas)\right)\right]\\
&=&\log\prob(x;\btheta)-\log \prob(x;\bthetas)-\KL\left(\setQ_1,p(\cdot,\cdot|\bx,\btheta)\right)+\underset{\setQ_2\in\mcQ}{\min}\;\KL\left(\setQ_2,p(\cdot,\cdot|\bx,\bthetas)\right)\\
&\leq&\log\prob(x;\btheta)-\log \prob(x;\bthetas)+\log\prob\left(\#\Symmetric(\bthetas)\right)\\
&\leq&\underbrace{\log\prob(x;\btheta)-\log \prob(x;\bthetas)}_{\underset{\n\dd\to+\infty}{\longrightarrow}-\infty}+\log\left(\g!\m!\right).\\
\end{eqnarray*}
Finally, we have $\balvar\approx\bals$ for $\n$ and $\dd$ enough larges and with the same arguments as the proof of the proposition~\ref{cor:behaviorEMV}, we have $\hat{\bal}\left(\bzs,\bws\right)-\balvar^{s,t}=o_{P}\left(1\right)$. 
\end{comment}
%\[\bthetavar\in\underset{\btheta}{\arg\!\max}\!\left(\underset{\setQ\in\mcQ}{\max}\Jvar\left(\setQ,\btheta\right)\right)\subset
%\underset{\btheta}{\arg\!\max}\log\prob(\bx;\btheta).\]

%Preuve de l'affirmation
%By the theorem~\ref{thm:observed-akin-to-complete-general}, we have
%\begin{eqnarray*}
%\log\prob(x;\btheta)-\log \prob(x;\bthetas)&=&\log\frac{\prob(x;\btheta)}{\prob(x;\bthetas)}\\
%&=&\log\left(\frac{\#\Symmetric(\btheta)}{\#\Symmetric(\bthetas)}\frac{\prob\left(\bx,\bzs,\bws;\btheta\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}\left(1+o_P(1)\right)+o_P(1)\right).\\
%\end{eqnarray*}

%But, we know that
%\begin{eqnarray*}
%\log\left(\frac{\prob\left(\bx,\bzs,\bws;\btheta\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}\right)&=&\log\left(\frac{\prob\left(\bx,\bzs,\bws;\btheta\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}\right)-D_{\n,\dd}(\btheta,\bthetas)+D_{\n,\dd}(\btheta,\bthetas)\\
%&=&\sum_{\kk=1}^{\g}{\left(\zssumk-\n\pisk\right)\log\left(\frac{\pik}{\pisk}\right)}+\sum_{\el=1}^{\m}{\left(\wssuml-\dd\rhosl\right)\log\left(\frac{\rhol}{\rhosl}\right)}\\
%& &+\sum_{\kk,\el}\left\{\left(\alkl-\alskl\right)\left[\zssumk\wssuml\hat{\bx}_{\kk,\el}\left(\bzs,\bws\right)-\n\dd\pisk\rhosl\Psi'\left(\alskl\right)\right]+\left(\Psi(\alkl)-\Psi(\alskl)\right)\left[\zssumk\wssuml-\n\dd\pisk\rhosl\right]\right\}\\
%&&-\n\sum_{\kk=1}^{\g}\pisk\log\left(\frac{\pik}{\pisk}\right)-\dd\sum_{\el=1}^{\m}\rhosl\log\left(\frac{\rhol}{\rhosl}\right)\\
%& &-\n\dd\sum_{\kk,\el}\pisk\rhosl\left[\Psi'(\alskl)\left(\alkl-\alskl\right)-\left(\Psi(\alkl)-\Psi(\alskl)\right)\right]\\
%\end{eqnarray*}



%and it remains to be demonstrated that
%\begin{equation}\label{eq:fracbthetas}
%\frac{\prob\left(\bx,\bz,\bw;\bthetas\right)}{\prob\left(\bx,\bzs,\bws;\bthetas\right)}=1+\smallO_{P}(1).
%\end{equation}
%To prove this, we begin to see that for every permutations $s$ and $t$, we have $\frac{\prob\left(\bx,\bz,\bw;{\bthetas}^{s,t}\right)}{\prob\left(\bx,\bzs,\bws;{\bthetas}^{s,t}\right)}=\smallO_P(1)$ and, in an equivalent manner, $\frac{\prob\left(\bx,s^{-1}\left(\bz\right),t^{-1}\left(\bw\right);\bthetas\right)} {\prob\left(\bx,s^{-1}\left(\bzs\right),t^{-1}\left(\bws\right);\bthetas\right)}=\smallO_P(1)$ and we obtain
%\[\sum_{\overset{\bz'\in\Sz\backslash\{\bz\}}{\underset{\bw'\in\Sw\backslash\{\bw\}}{}}} \frac{\prob\left(\bx,\bz',\bw';\bthetas\right)} {\prob\left(\bx;\bthetas\right)} \leq\sum_{\overset{\bz'\in\Sz\backslash\{\bz\}}{\underset{\bw'\in\Sw\backslash\{\bw\}}{}}} \frac{\prob\left(\bx,\bz',\bw';\bthetas\right)} {\prob\left(\bx,\bzs,\bws;\bthetas\right)}=\smallO_{P}(1)\]
%where $\Sz$ denoting the set of the matrix $\bz^{s}$ where $s$ is a permutation. Given $\bx,\;\bz$ and $\bw$ generated by $\bthetas$, we know by \citet{mariadassou2012convergence}\textcolor{red}{[A reverifier]} that there exist a positive constant $C_{\bthetas}$ such that
%\[\underset{\n,\dd}{\lim}\frac{\ln\Prob\left(\zx\notin\Szs,\wx\notin\Sws\right)}{\sqrt{\n\dd}}\leq-C_{\bthetas}<0\]
%where $\left(\zx,\wx\right)$ denote the maximum profile likelihood estimate of $\left(\bz,\bw\right)$, that is, the set $\arg\!\max_{\btheta}\prob\left(\bx,\bz;\btheta\right)$. This implies that $\Prob\left(\zx\in\Szs,\wx\in\Sws\right)=o(1)$ and, by the Markov's inequality, this implies that
%\[\Prob\left(\left.\zx\notin\Szx,\wx\notin\Swx\right|\bx\right)=\smallO_{P}(1)\]
%and we obtain
%\[\sum_{\overset{\bz'\notin\Sz}{\underset{\text{or }\bw'\notin\Sw\backslash\{\bw\}}{}}} \frac{\prob\left(\bx,\bz',\bw';\bthetas\right)} {\prob\left(\bx;\bthetas\right)}=\smallO_{P}(1)\]
%and we have the result~(\ref{eq:fracbthetas}) combining the two sums.\\
%
%Thus, $\prob\left(\bx,\bz,\bw;\bthetas\right)=\prob\left(\bx,\bzs,\bws;\bthetas\right)\left(1+\smallO_{P}(1)\right)$, we obtain
%\[\frac{\# \Symmetric(\btheta)}{\# \Symmetric(\bthetas)}\underset{\btheta' \sim \btheta}{\max}\frac{\prob\left(\bx,\bz,\bw;\btheta'\right)}{\prob\left(\bx,\bz,\bw;\bthetas\right)}= \underset{\bz'\in\Sz;\bw'\in\Sw}{\max}\frac{\prob\left(\bx,\bz',\bw';\btheta\right)}{\prob\left(\bx,\bz,\bw;\bthetas\right)}\leq \frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]} {\prob\left(\bx,\bzs,\bws;\bthetas\right)\left(1+\smallO_{P}(1)\right)}.\]
%On the other hand, we had
%\[\frac{\underset{\setQ\in\mcQ}{\max}\;\exp\left[\Jvar\left(\setQ,\btheta\right)\right]} {\prob\left(\bx,\bzs,\bws;\bthetas\right)} \leq\frac{\prob\left(\bx,\bzs,\bws;\btheta\right)} {\prob\left(\bx,\bzs,\bws;\bthetas\right)}\leq \frac{\# \Symmetric(\btheta)}{\# \Symmetric(\bthetas)} \max_{\btheta' \sim \btheta} \frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \bthetas)}\left(1 + \smallO_P(1)\right) + \smallO_P(1).\]
%
%Combining the two bounds give the result of the corollary~\ref{cor:Variational}.




%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Technical Lemma}
\label{sec:technical-lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\subsection{Proof of Lemma~\ref{cor:prob-regular-configurations-star}}
\CK{A laisser? ou Ã  mettre dans le texte?}
\proofbegin
Each $\zsumk$ is a sum of $\n$ i.i.d Bernoulli r.v. with parameter $\pik \geq \pii_{\min} \geq c$. A simple Hoeffding bound shows that
\begin{equation*}
  \Prob_{\bthetas}\left( \zsumk \leq \n \frac{c}{2} \right)
  \leq
  \Prob_{\bthetas}\left( \zsumk \leq \n \frac{\pik}{2} \right)
  \leq
  \exp\left( - 2\n\left(\frac{\pik}{2}\right)^2 \right)
  \leq
  \exp\left( - \frac{\n c^2}{2} \right)
\end{equation*}
Similarly,
\begin{equation*}
  \Prob_{\bthetas}\left( \wsuml \leq \dd \frac{c}{2} \right) \leq \exp\left( - \frac{\dd c^2}{2} \right)
\end{equation*}
The proposition follows from a union bound over $\g$ values of $\kk$ and $\m$ values of $\el$.
\proofend
\end{comment}
\subsection{Sub-exponential variables}
%% Subexponential variables
We now prove two propositions regarding subexponential variables. 
Recall first that a random variable $X$ is  sub-exponential with parameters $(\tau^2, b)$ if for all $\lambda$ such that $|\lambda|\leq 1/b$,
\[
\Esp[e^{\lambda( X-\Esp(X))}] \leq \exp\left( \frac{\lambda^2 \tau^2}{2} \right).
\]
In particular, all distributions coming from a natural exponential family are sub-exponential. Sub-exponential variables satisfy a large deviation Bernstein-type inequality:
\begin{equation}
  \label{eq:concentration-subexponential}
  \Prob( X - \Esp[X] \geq t) \leq 
  \begin{cases} 
    \exp\left( - \frac{t^2}{2 \tau^2}\right)  & \text{if} \quad 0 \leq t \leq \frac{\tau^2}{b} \\ 
    \exp\left( - \frac{t}{2b}\right)  & \text{if} \quad t \geq \frac{\tau^2}{b}
  \end{cases}
\end{equation}
So that
\[
\Prob( X - \Esp[X] \geq t) \leq \exp\left( - \frac{t^2}{2(\tau^2+bt)}\right)
\]
The subexponential property is preserved by summation and multiplication. 
\begin{itemize}
\item If $X$ is sub-exponential with parameters $(\tau^2, b)$ and $\alpha \in \mathbb{R}$, then so is $\alpha X$ with parameters $(\alpha^2\tau^2, \alpha b)$
\item If the $X_i$, $i = 1,\dots,n$ are sub-exponential with parameters $(\tau_i^2, b_i)$ and independent, then so is $X = X_1 + \dots + X_n$ with parameters $(\sum_i \tau_i^2,\max_i b_i)$
\end{itemize}


\begin{proposition}[Maximum in $(\bz, \bw)$] \label{proposition:maxzw}
  Let $(\bz, \bw)$ be a configuration and $\hat{x}_{\kk, \el}(\bz, \bw)$ resp. $\barxkl(\bz, \bw)$ be as defined in Equations~\eqref{eq:mle-complete-likelihood} and~\eqref{eq:profile-likelihood-notations}. Under the assumptions of the section~\ref{sec:assumptions}, for all $\vareps>0$
  \begin{equation}
    \label{eq:ineg-de-bernst-1}
    \Prob \left(\max_{\bz, \bw} \max_{k,l} \pizk \rhowl |\hat{x}_{\kk, \el} - \barxkl| > \vareps \right)  \leq  \g^{\n+1} \m^{\dd + 1} \exp \left( - \frac{nd\vareps^2}{2(\bar{\sigma}^2 + \neighborsize^{-1}\vareps)}\right). \\
  \end{equation}
  Additionally, the suprema over all $c/2$-regular  assignments satisfies:
  \begin{equation}
    \label{eq:ineg-de-bernst-2}
    \Prob \left(\max_{\bz \in \mcZ_1, \bw \in \mcW_1} \max_{k,l} |\hat{x}_{\kk, \el} - \barxkl| > \vareps \right) \leq  \g^{\n+1} \m^{\dd + 1} \exp \left( - \frac{ndc^2\vareps^2}{8(\bar{\sigma}^2 + \neighborsize^{-1}\vareps)}\right). \\
  \end{equation}
\end{proposition}
Note that equations \ref{eq:ineg-de-bernst-1} and \ref{eq:ineg-de-bernst-2} remain valid when replacing $c/2$ by any $\tilde c < c/2
$.
\proofbegin

The random variables $X_{ij}$ are subexponential with parameters $(\vmax^2, 1/\neighborsize)$. Conditionally to $(\bzs, \bws)$, $\zsumk \wsuml(\hat{x}_{\kk, \el} - \barxkl)$ is a sum of $\zsumk \wsuml$ centered subexponential random variables. By Bernstein's inequality \cite{massart2007concentration}, we therefore have for all $t > 0$
\begin{align*}
\Prob( \zsumk \wsuml | \hat{x}_{\kk, \el} - \barxkl| \geq t ) & \leq 2 \exp\left( - \frac{t^2}{2(\zsumk \wsuml\bar{\sigma}^2 + \neighborsize^{-1} t)}\right) \\
\end{align*}
In particular, if $t = \n\dd x$,
\begin{align*}
\Prob\left( \pizk \rhowl | \hat{x}_{\kk, \el} - \barxkl| \geq x \right) & \leq 2 \exp\left( - \frac{ndx^2}{2(\pizk\rhowl\bar{\sigma}^2 + \neighborsize^{-1} x)}\right) \leq 2 \exp\left( - \frac{ndx^2}{2(\bar{\sigma}^2 + \neighborsize^{-1} x )}\right) \\
\end{align*}
uniformly over $(\bz, \bw)$. Equation~\eqref{eq:ineg-de-bernst-1} then results from a union bound.
Similarly,
\begin{align*}
  \Prob\left( | \hat{x}_{\kk, \el} - \barxkl| \geq x \right) & = \Prob\left( \pizk \rhowl | \hat{x}_{\kk, \el} - \barxkl| \geq \pizk \rhowl x \right) \\
  & \leq 2 \exp\left( - \frac{ndx^2\pizk^2\rhowl^2}{2(\pizk\rhowl\bar{\sigma}^2 + \neighborsize^{-1} x \pizk\rhowl)}\right) \\
  & \leq 2 \exp\left( - \frac{ndc^2x^2}{8(\bar{\sigma}^2 + \neighborsize^{-1} x )}\right)
\end{align*}
Where the last inequality comes from the fact that $c/2$-regular assignments satisfy $\pizk \rhowl \geq c^2/4$. Equation~\eqref{eq:ineg-de-bernst-2} then results from a union bound over $\mcZ_1 \times \mcW_1 \subset \mcZ \times \mcW$.
\proofend

\begin{lemme}\label{lemme:absolutesubexponential}\textbf{}
If $X$ is a zero mean random variable, subexponential with parameters $(\sigma^2, b)$, then $|X|$ is subexponential with parameters $(8\sigma^2, 2\sqrt{2}b)$.
\end{lemme}

\proofbegin
Note $\mu = \Esp|X|$ and consider $Y = |X| - \mu$. Choose $\lambda$ such that $|\lambda| < (2\sqrt{2}b)^{-1}$. We need to bound $\Esp[e^{\lambda Y}]$. Note first that $\Esp[e^{\lambda Y}] \leq \Esp[e^{\lambda X}] + \Esp[e^{-\lambda X}] < +\infty$ is properly defined by subexponential property of $X$ and we have
\[ 
\Esp[e^{\lambda Y}] \leq 1 + \sum_{k=2} \frac{|\lambda|^k\Esp[|Y|^k]}{k!} 
\]
where we used the fact that $\Esp[Y] = 0$. We know bound odd moments of $|\lambda Y|$. 
\[
\Esp[|\lambda Y|^{2k+1}] \leq (\Esp[|\lambda Y|^{2k}]\Esp[|\lambda Y|^{2k+2}])^{1/2} \leq \frac{1}{2} (\lambda^{2k} \Esp[Y^{2k}] + \lambda^{2k+2} \Esp[Y^{2k+2}])
\]
where we used first Cauchy-Schwarz and then the arithmetic-geometric mean inequality. The Taylor series expansion can thus be reduced to 
\begin{align*}
\Esp[e^{\lambda Y}] & \leq 1 + \left(\frac{1}{2} + \frac{1}{2.3!}\right) \Esp[Y^2] \lambda^2+ \sum_{k=2}^{+\infty} \left( \frac{1}{(2k)!} + \frac{1}{2}\left[ \frac{1}{(2k-1)!} + \frac{1}{(2k+1)!} \right] \right)\lambda^{2k} \Esp[Y^{2k}] \\ 
& \leq \sum_{k=0}^{+\infty} 2^k \frac{\lambda^{2k}\Esp[Y^{2k}]}{(2k)!} \\
&\leq \sum_{k=0}^{+\infty} 2^{3k} \frac{\lambda^{2k}\Esp[X^{2k}]}{(2k)!} =\cosh\left(2\sqrt{2}\lambda X\right) 
=\Esp\left[ \frac{e^{2\sqrt{2}\lambda X} + e^{-2\sqrt{2}\lambda X}}{2}\right]\\
& \leq e^{\frac{8\lambda^2\sigma^2}{2}}\\
\end{align*}
where we used the well-known inequality $\Esp[|X - \Esp[X]|^k] \leq 2^k \Esp[|X|^k]$ to substitute $2^{2k}\Esp[X^{2k}]$ to $\Esp[Y^{2k}]$.\\


\proofend


\begin{proposition}[concentration for subexponential]
\label{prop:concentration-subexponential}
Let $\X_{1}, \dots, \X_{\n}$ be independent zero mean random variables, subexponential with parameters $(\sigma_i^2, b_i)$. Note $V_0^2  = \sum_{\ii} \sigma_i^2$ and $b = \max_{i} b_i$. Then the random variable $Z$ defined by:
\begin{equation*}
  Z = \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}} \sum_{\ii} \Gamma_i X_{\ii}
\end{equation*}
is also subexponential with parameters $(8 M^2 V_0^2, 2\sqrt{2} Mb)$. 
% and the following Bernstein inequality holds:
% \begin{equation*}
%   \Prob( Z - \Esp[Z] \geq t) \leq 
%   \begin{cases} 
%     \exp\left( - \frac{t^2}{2 (M^2 V_0^2+n \log(2))}\right)  & \text{if} \quad 0 \leq t \leq \frac{M^2 V_0^2 + \n \log(2)}{Mb} \\ 
%     \exp\left( - \frac{t}{2Mb}\right)  & \text{if} \quad t \geq \frac{M^2 V_0^2 + \n \log(2)}{Mb}
%   \end{cases}
% \end{equation*}
Moreover $\Esp[Z] \leq M V_0 \sqrt{\n}$ so that for all $t > 0$, 
\begin{equation}
  \label{eq:concentration-subexponential}
\Prob( Z - M V_0 \sqrt{n} \geq t) \leq \exp \left( - \frac{t^2}{2 (8M^2 V_0^2 + 2\sqrt{2} M b t)}\right)
\end{equation}

\end{proposition}

\proofbegin
Note first that $Z$ can be simplified to $Z =  M \sum_{\ii} |X_{\ii}|$. We just need to bound bound $\Esp[Z]$. The rest of the proposition results from the fact that the $|X_{\ii}|$ are subexponential $(8\sigma_{\ii}^2, 2\sqrt{2}b_{\ii})$ by Lemma~\ref{lemme:absolutesubexponential} and standard properties of sums of independent rescaled subexponential variables. 
\begin{align*}
  \Esp[Z] & = \Esp \left[ \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}}  \sum_{\ii} \Gamma_{\ii} X_{\ii} \right]  = \Esp \left[ \sum_{\ii} M |X_{\ii}| \right] \leq M \sum_{\ii} \sqrt{\Esp [ X_{\ii}^2 ]} \\ & 
  = M \sum_{\ii} \sigma_i \leq M \left( \sum_{\ii} 1 \right)^{1/2} \left( \sum_{\ii} \sigma_{\ii}^2 \right)^{1/2} = M V_0\sqrt{\n}
\end{align*}
using Cauchy-Schwarz. 
% 
% 
% 
% 
% We simply need to bound $\Esp[Z]$ and to show that $Z$ is sub-exponential. Equation~\eqref{eq:concentration-subexponential} results from properties of sub-exponential random variables. Choose  $\lambda \in \R$ such that $|\lambda| \leq 1/( Mb )$: 
% \begin{align*}
% \Esp[\exp(\lambda Z)] & = \Esp\left[ \exp\left( \lambda \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}} \sum_{\ii} \Gamma_{\ii} X_{\ii} \right)\right] \\
% & \leq \Esp\left[ \prod_{\ii} \exp\left( \lambda \sup_{| \Gamma_{\ii} | \leq M}  \Gamma_{\ii} X_{\ii} \right)\right] \\
%  & \leq \prod_{\ii}  \Esp\left[  \exp\left( \lambda M X_{\ii} \right) + \exp\left( -\lambda M X_{\ii} \right)\right] \\
%  & \leq \prod_{\ii}  2\exp\left( \frac{M^2 \lambda^2 \sigma_{\ii}^2}{2}\right) = \exp\left( \frac{M^2 \lambda^2 V_0^2 + \n \log(2)}{2}\right)
% \end{align*}
% where the last inequality comes from the sub-exponential nature of the $X_{\ii}$. This proves that $Z$ is sub-exponential with parameters $(M^2 V_0^2 + \n \log(2), Mb)$. \\
% \CK{ non, il faudrait avoir
% $\exp\left( \frac{M^2  V_0^2 + \n \log(2)}{2}\lambda^2 \right)$. En fait, la majoration $ \exp\left( \lambda M X_{\ii} \right) + \exp\left( -\lambda M X_{\ii} \right)$ est trop forte.}
% 
% 
% We now bound $\Esp[Z]$. 
% \begin{align*}
%   \Esp[Z] & = \Esp \left[ \sup_{\substack{\Gamma \in \R^{\n} \\ \|\Gamma \|_{\infty} \leq M}}  \sum_{\ii} \Gamma_{\ii} X_{\ii} \right]  \leq \Esp \left[ \sum_{\ii} M |X_{\ii}| \right] \leq M \sum_{\ii} \sqrt{\Esp [ X_{\ii}^2 ]} \\ & 
%   = M \sum_{\ii} \sigma_i \leq M \left( \sum_{\ii} 1 \right)^{1/2} \left( \sum_{\ii} \sigma_{\ii}^2 \right)^{1/2} = M V_0\sqrt{\n}
% \end{align*}
% using Cauchy-Schwarz. 
\proofend

%% Lemma on symmetric configurations
% \subsection{Symmetric and equivalent configurations}
% The next lemma deals with the contributions of symmetric and equivalent configurations to the observed likelihood. 
% 
% \begin{lemme}
%   \label{lem:symmetry}
%   For all $\btheta \in \bTheta$, Consider the function 
%   \begin{equation*}
%     H_{\n\dd}(\btheta) = \n \KL(\bpis, \bpi) + \dd \KL(\brhos, \brho) + \n\dd \sum_{\kk,\el} \pik^\vrai \rhol^\vrai \KL(\alskl, \alkl')
%   \end{equation*}
%   Then
%   \begin{equation*}
%     \sum_{(\bz, \bw) \sim (\bzs, \bws)} \prob(\bx, \bz, \bw; \btheta) = \# \Symmetric(\btheta) \# \argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta') \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') (1 + \smallO_P(1))
%   \end{equation*}
%   where the $\smallO_P$ is uniform in $\btheta$. And specifically for $\bthetas$, 
%   \begin{equation*}
%     \sum_{(\bz, \bw) \sim (\bzs, \bws)} \prob(\bx, \bz, \bw; \bthetas) = \# \Symmetric(\bthetas) \prob(\bx, \bzs, \bws; \bthetas) (1 + \smallO_P(1))
%   \end{equation*}
% \end{lemme}
% 
% \proofbegin
% Choose $(s, t)$ permutations of $\{1, \dots, \g\}$ and $\{1, \dots, \m\}$ and assume that $\bz = \bz^{\vrai,s}$ and $\bw = \bw^{\vrai, t}$. Then $\prob(\bx, \bz, \bw; \btheta) = \prob(\bx, \bz^{\vrai, s}, \bw^{\vrai, t}; \btheta) = \prob(\bx, \bzs, \bws; \btheta^{s,t})$.  If furthermore $(s, t) \in \Symmetric(\btheta)$, $\btheta^{s, t} = \btheta$ and immediately $\prob(\bx, \bz, \bw; \btheta) = \prob(\bx, \bzs, \bws; \btheta)$. We can therefore partition the sum as 
% 
% \begin{align*}
%   \sum_{(\bz, \bw) \sim (\bz, \bw)} \prob(\bx, \bz, \bw; \btheta) & = \sum_{s, t} \prob(\bx, \bz^{\vrai, s}, \bw^{\vrai, t}; \btheta) \\ 
%   & = \sum_{s, t} \prob(\bx, \bzs, \bws; \btheta^{s,t}) \\ 
%   & = \sum_{\btheta' \sim \btheta} \# \Symmetric(\btheta') \prob(\bx, \bzs, \bws; \btheta') \\ 
%   & = \# \Symmetric(\btheta) \sum_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') \\ 
% \end{align*}
% 
% We can easily show that 
% \begin{equation*}
%   \log \frac{\prob(\bx, \bzs, \bws; \btheta)}{\prob(\bx, \bzs, \bws; \bthetas)} = - H_{\n\dd}(\btheta) ( 1 + \smallO_P(1))
% \end{equation*}
% where the $\smallO_P$ is uniform over $\bTheta$. For $\btheta$ fixed, since there are at most $\g^\g \m^\m$ $\btheta'$ that are equivalent to $\btheta$, $\prob(\bx, \bzs, \bws; \btheta')$ reaches its maximum on an element of $\argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta')$ with high probability for $\n$ and $\dd$ large enough. 
% 
% Assume without loss of generality that $\btheta \in \argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta')$ and realizes the maximum of $\prob(\bx, \bzs, \bws; \btheta')$ over $\btheta' \sim \btheta$. Choose $\btheta'$ not in the $\argmax$. 
% \begin{align*}
% \log \frac{\prob(\bx, \bzs, \bws; \btheta')}{\prob(\bx, \bzs, \bws; \btheta)} & = \left( H_{\n\dd}(\btheta) - H_{\n\dd}(\btheta') \right) (1 + \smallO_P(1)) \xrightarrow[\n,\dd \to +\infty]{} -\infty
% \end{align*}
% so that 
% \begin{align*}
%   \sum_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') = \max_{\btheta' \sim \btheta} \prob(\bx, \bzs, \bws; \btheta') \# \argmax_{\btheta' \sim \btheta} H_{\n\dd}(\btheta') \left(1 + \smallO_P(1) \right)
% \end{align*}
% which proves the first equation. The second comes from the fact that 
% \begin{equation*}
%   \argmax_{\btheta \sim \bthetas} H_{\n\dd}(\btheta) = \{ \bthetas \} \quad \text{and} \quad \Prob_{\bthetas}\left( \max_{\btheta \sim \bthetas} \prob(\bx, \bzs, \bws; \btheta) \neq \prob(\bx, \bzs, \bws; \bthetas) \right) = \smallO(1)
% \end{equation*}
% \proofend

The final lemma is the working horse for proving Proposition~\ref{prop:maximum-conditional-likelihood}. 

\begin{lemme}\label{lemme:casdegalite}\textbf{}

Let $\etaa$ and $\bar{\etaa}$ be two matrices from $M_{\g \times \m}(\Theta)$ and $f: \Theta \times \Theta \to \mathbb{R_+}$ a positive function, $A$ a (squared) confusion matrix of size $\g$ and $B$ a (squared) confusion matrix of size $\m$. We denote $D_{\kk\el\kk'\el'} = f(\etaa_{\kk\el}, \bar{\etaa}_{\kk'\el'})$. Assume that
\begin{itemize}
\item all the rows of $\etaa$ are distinct;
\item all the columns $\etaa$ are distinct;
\item $f(x,y) = 0 \Leftrightarrow x = y$;
\item each  row of $A$ has a non zero element;
\item each  row of $B$ has a non zero element;
\end{itemize}
and note
\begin{equation}
\label{eq:casdegalite}
\Sigma = \sum_{\kk \kk'} \sum_{\el \el'} A_{\kk \kk'} B_{\el \el'} d_{\kk\el\kk'\el'}
\end{equation}
Then,
\begin{equation*}
  \Sigma = 0 \Leftrightarrow \begin{cases}
    A, B \text{ are permutation matrices } s,t & \\
    \bar{\etaa} = \etaa^{s, t} \text{ cad } \forall (\kk, \el), \bar{\etaa}_{\kk\el} = \etaa_{s(\kk) t(\el)} &
  \end{cases}
\end{equation*}
\end{lemme}
\proofbegin
If $A$ and $B$ are the permutation matrices corresponding to the permutations $s$ et $t$: $A_{ij} = 0$ if $i \neq s(j)$ and $B_{ij} = 0$ if $i \neq t(j)$. As each row of $A$ contains a non zero element  and as $A_{s(\kk)\kk} > 0$ (resp. $B_{s(\el)\el} > 0$) for all $\kk$ (resp. $\el$), the following sum $\Sigma$ reduces to
\begin{equation*}
  \Sigma = \sum_{\kk \kk'} \sum_{\el \el'} A_{\kk \kk'} B_{\el \el'} d_{\kk\el\kk'\el'} = \sum_{\kk} \sum_{\el} A_{s(\kk)\kk} B_{t(\el)\el} d_{s(\kk)t(\el) \kk\el}
\end{equation*}
$\Sigma$ is null and sum of positive components, each component is null. However, all $A_{s(\kk)\kk}$ and $B_{t(\el)\el}$ are not null, so that for all $(\kk, \el)$, $d_{s(\kk)t(\el) \kk\el} = 0$ and $\bar{\etaa}_{\kk\el} = \etaa_{s(\kk) t(\el)}$.\\
%
Now, if $A$ is not a permutation matrix while $\Sigma = 0$ (the same reasoning holds for $B$ or both). Then $A$ owns a column $k$ that contains two non zero elements, say $A_{\kk_1 \kk}$ and $A_{\kk_2 \kk}$. Let $\el \in \{1\dots\m\}$, there exists by assumption $\el'$ such that $B_{\el \el'} \neq 0$. As $\Sigma=0$, both products $A_{\kk_1 \kk} B_{\el \el'} d_{\kk_1 \el \kk \el'}$ and $A_{\kk_2 \kk} B_{\el \el'} d_{\kk_2 \el \kk \el'}$ are zero.
\begin{equation*}
  \begin{cases}
    A_{\kk_1 \kk} B_{\el \el'} d_{\kk_1 \el \kk \el'} & = 0 \\
    A_{\kk_2 \kk} B_{\el \el'} d_{\kk_2 \el \kk \el'} & = 0 \\
  \end{cases}
  \Leftrightarrow
  \begin{cases}
    d_{\kk_1 \el \kk \el'} & = 0 \\
    d_{\kk_2 \el \kk \el'} & = 0 \\
  \end{cases}
  \Leftrightarrow
  \begin{cases}
    \etaa_{\kk_1 \el} = \bar{\etaa}_{\kk \el'} & \\
    \etaa_{\kk_2 \el} = \bar{\etaa}_{\kk \el'} & \\
  \end{cases}
  \Leftrightarrow
  \etaa_{\kk_1 \el} = \etaa_{\kk_2 \el}
\end{equation*}
The previous equality is true for all $\el$, thus rows $\kk_1$ and $\kk_2$ of $\etaa$ are identical, and contradict the assumptions. \proofend 
%
%La rÃ©ciproque est immÃ©diate.
%\proofend 

%% Lemma for log likelihood ratio of assignments
\subsection{Likelihood ratio of assignments}

\begin{lemme}\label{lemme:marginalprobabilties}\textbf{}

Let $\mcZ_1$ be the subset of $\mcZ$ of $c$-regular configurations, as defined in Definition~\ref{def:regular}. 
Let $\mathbb{S}^{\g} = \{ \bpi = (\pii_1, \pii_2, \dots, \pii_\g) \in [0, 1]^\g : \sum_{k=1}^\g \pii_k = 1\}$ be the $g$-dimensional simplex and
note $\mathbb{S}^{\g}_c = \mathbb{S}^{\g} \cap [c, 1-c]^\g$. Then there exists two positive constants $M_c$ and $M'_c$ such that for all $\bz$, $\bzs$ in $\mcZ_1$ and all $\bpi \in \mathbb{S}_c^{g}$
\begin{eqnarray*}
\left| \log \prob(\bz; \hat{\bpi}(\bz)) - \log \prob(\bzs; \hat{\bpi}(\bzs)) \right| & \leq & M_c \|\bz - \bzs \|_0 
\end{eqnarray*}
\end{lemme}

\proofbegin
Consider the entropy map $H: \mathbb{S}^{g} \to \mathbb{R}$ 
defined as $H(\bpi) = - \sum_{k = 1}^{\g} \pii_k \log(\pii_k)$. The gradient $\nabla H$ is uniformly bounded by $\frac{M_c}{2} = \log \frac{1-c}{c}$ in $\|.\|_{\infty}$-norm over $\mathbb{S}^{\g} \cap [c, 1-c]^\g$. 
Therefore, for all $\bpi$, $\bpis \in \mathbb{S}^{\g} \cap [c, 1-c]^\g$, we have 
\begin{equation*}
| H(\bpi) - H(\bpis) | \leq \frac{M_c}{2} \| \bpi -\bpis \|_1
\end{equation*}
To prove the  inequality, we remark that $\bz \in \mcZ_1$ translates to $\hat{\bpi}(\bz) \in \mathbb{S}^{\g} \cap [c, 1-c]^\g$, 
that $\log \prob(\bz; \hat{\bpi}(\bz)) - \log \prob(\bzs; \hat{\bpi}(\bzs)) = n [ H(\hat{\bpi}(\bz)) - H(\hat{\bpi}(\bzs)) ]$ 
and finally that $\|\hat{\bpi}(\bz) - \hat{\bpi}(\bzs) \|_1 \leq \frac{2}{n} \| \bz - \bzs \|_0$. 
\proofend


\begin{corollaire}\label{cor:marginalprobabilties}\textbf{}
Let $\bzs$ (resp. $\bws$) be $c/2$-regular and $\bz$ (resp. $\bw$) at $\|.\|_0$-distance $c/4$ of $\bzs$ (resp. $\bws$). Then, for all $\btheta \in \bTheta$
\begin{equation*}
\log \frac{\prob(\bz, \bw; \btheta)}{\prob(\bzs, \bws; \bthetas)} \leq \bigO_P(1) \exp \left\{ M_{c/4} ( \|\bz - \bzs \|_0 + \|\bw - \bws \|_0 ) \right\} \\ 
% & \times \exp \left\{ M'_{c/2} ( \n \|\bpi(\bzs) - \bpis \|_1 + \dd \|\brho(\bws) - \brhos \|_1 ) \right\}
\end{equation*}
\end{corollaire}

\proofbegin
Note then that:
\begin{align*}
\frac{\prob(\bz, \bw; \btheta)}{\prob(\bzs, \bws; \bthetas)} & = & \frac{\prob(\bz, \bw; \bpi, \brho)}{\prob(\bzs, \bws; \bpis, \brhos)} = 
\frac{\prob(\bz, \bw; \bpi, \brho)}{\prob(\bzs, \bws; \hat{\bpi}(\bzs), \hat{\brho}(\bws))} \frac{\prob(\bzs, \bws; \hat{\bpi}(\bzs), \hat{\brho}(\bws))}{\prob(\bzs, \bws; \bpis, \brhos)} \\
& \leq & \frac{\prob(\bz, \bw; \hat{\bpi}(\bz), \hat{\brho}(\bw))}{\prob(\bzs, \bws; \hat{\bpi}(\bzs), \hat{\brho}(\bws))} \frac{\prob(\bzs, \bws; \hat{\bpi}(\bzs), \hat{\brho}(\bws))}{\prob(\bzs, \bws; \bpis, \brhos)} \\
& \leq & \exp \left\{ M_{c/4} ( \|\bz - \bzs \|_0 + \|\bw - \bws \|_0 ) \right\} \times \frac{\prob(\bzs, \bws; \hat{\bpi}(\bzs), \hat{\brho}(\bws))}{\prob(\bzs, \bws; \bpis, \brhos)} \\
% & \leq & \exp \left\{ M_{c/4} ( \|\bz - \bzs \|_0 + \|\bw - \bws \|_0 ) \right\} \\ 
% & & \times \exp \left\{ M'_{c/2} ( \n \|\bpi(\bzs) - \bpis \|_1 + \dd \|\brho(\bws) - \brhos \|_1 ) \right\}
& \leq & \bigO_P(1) \exp \left\{ M_{c/4} ( \|\bz - \bzs \|_0 + \|\bw - \bws \|_0 ) \right\}  \\
\end{align*}
where the first inequality comes from the definition of $\hat{\bpi}(\bz)$ and $\hat{\brho}(\bw)$ and the second from Lemma~\ref{lemme:marginalprobabilties} and the fact that $\bzs$ and $\bz$ (resp. $\bws$ and $\bw$) are $c/4$-regular. 
% \textcolor{red}{and the last from Lemma~\ref{lemme:marginalprobabilties} and the fact that $\hat{\bpi}(\bzs)$ and $\bpis$ (resp. $\hat{\brho}(\bws)$ and $\brhos$) are in $\mathbb{S}^{\g}_{c/2}$ (resp. $\mathbb{S}^{\m}_{c/2}$).}
Finally, local asymptotic normality of the MLE for multinomial proportions ensures that $\frac{\prob(\bzs, \bws; \hat{\bpi}(\bzs), \hat{\brho}(\bws))}{\prob(\bzs, \bws; \bpis, \brhos)} = \bigO_P(1)$. 
\proofend



%%%%%%%%%%
%\section{Proofs regarding the estimators}
